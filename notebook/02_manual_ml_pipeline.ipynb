{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeting Direct Marketing Model Training ML Pipeline [manual]\n",
    "\n",
    "---\n",
    "\n",
    "Once you are familiar with using Amazon SageMaker built-in algorithm - [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to do [Targetting Direct Marketing model traing](./01_xgboost_direct_marketing_sagemaker.ipynb), we are going to build a ML Pipeline to automate the workflow with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). \n",
    "\n",
    "In the design:\n",
    "* Preprocessing Job for feature engineering\n",
    "* Model training with tuned hyperparameters\n",
    "  * For example, you may collect the hyperparameters from HPO jobs with the best candidate.\n",
    "* Hyperparameters optimization is optional\n",
    "\n",
    "In the notebook, we are going to demo how to create the workflow step by step. Below is the related Step Functions workflow mapping to the ML pipeline with no HPO and using an trained model:\n",
    "\n",
    "![Direct Marketing](./images/dm_ml_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Creation\n",
    "---\n",
    "To create ML pipeline, we will use Step Functions Data Science SDK v2.0.0rc1, which is compatible with SageMaker SDK 2.x.\n",
    "\n",
    "We will cover pipeline creation at below:\n",
    "* Environment initialization\n",
    "* Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" # 2.0.0\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0rc1\"\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.ml_pipeline_dependencies import *\n",
    "\n",
    "model_name = \"direct-marketing-model-training\"\n",
    "# create bucket for model training\n",
    "bucket_name = f'{model_name}-{region}-{account_id}'\n",
    "try:\n",
    "    s3 = boto3.client('s3', region_name = region)\n",
    "    if 'us-east-1' == region:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "setup_trained_model(bucket_name, S3_KEY_TRAINED_MODEL)\n",
    "EXISTING_MODEL_URI = f\"s3://{bucket_name}/{S3_KEY_TRAINED_MODEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO tuning job notification. \n",
    "email_id = \"yuashi@amazon.com\"\n",
    "topic_name = 'dm-mt-notification-topic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the workflow execution role. For the role arn, please refer to the output tab of the CloudFormation stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_NAME = f'{model_name}-pipeline'\n",
    "ssm = boto3.client('ssm')\n",
    "response = ssm.get_parameter(Name = \"/ml_pipeline/workflow_execution_role\")\n",
    "WORKFLOW_EXECUTION_ROLE = response['Parameter']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_EXECUTION_ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WORKFLOW_EXECUTION_ROLE:\n",
    "    raise Exception(\"ML Pipeline Parameters in System Manager is not setup properly. Please check whether the ml-pipeline stack has been created or not.\")\n",
    "else:\n",
    "    print(f\"Workflow execution IAM service role: {WORKFLOW_EXECUTION_ROLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./bank-additional.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_input_path = f's3://{bucket_name}/preprocessing/input'\n",
    "local_data_file = './bank-additional/bank-additional-full.csv'\n",
    "sagemaker.s3.S3Uploader.upload(local_data_file, processing_input_path, sagemaker_session = sagemaker_session)\n",
    "\n",
    "input_data = f'{processing_input_path}/bank-additional-full.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)\n",
    "\n",
    "---\n",
    "\n",
    "#### Create Processing Step for data preprocessing\n",
    "\n",
    "We will now create the [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) that will launch a SageMaker Processing Job.\n",
    "\n",
    "In the processing job script `./pipeline/preprocessing.py`, the actions will be done:\n",
    "\n",
    "* Feature engineering on the dataset\n",
    "* Split training and test data \n",
    "* Store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to upload `preprocessing.py' code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_preprocess_code(bucket_name):\n",
    "    PREPROCESSING_SCRIPT_LOCATION = \"./pipeline/preprocessing.py\"\n",
    "    input_code_uri = sagemaker_session.upload_data(\n",
    "        PREPROCESSING_SCRIPT_LOCATION,\n",
    "        bucket = bucket_name,\n",
    "        key_prefix = \"preprocessing/code\",\n",
    "    )\n",
    "    return input_code_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_step(\n",
    "    processing_job_placeholder,\n",
    "    input_code_uri,\n",
    "    bucket_name,\n",
    "    data_file,\n",
    "    experiment_name,\n",
    "    trial_name,\n",
    "    sagemaker_execution_role\n",
    "):\n",
    "    preprocessing_processor = SKLearnProcessor(\n",
    "        framework_version='0.20.0',\n",
    "        role = sagemaker_execution_role,\n",
    "        instance_count = 1,\n",
    "        instance_type = 'ml.m5.xlarge',\n",
    "        max_runtime_in_seconds = 1200\n",
    "    )\n",
    "\n",
    "    processing_input_data = f's3://{bucket_name}/preprocessing/input/{data_file}'\n",
    "    inputs = [\n",
    "        ProcessingInput(\n",
    "            input_name = \"code\",\n",
    "            source = input_code_uri,\n",
    "            destination = \"/opt/ml/processing/input/code\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name = \"input_data\",\n",
    "            source = processing_input_data,\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    processing_output_data = f\"s3://{bucket_name}/preprocessing/output\"\n",
    "    outputs = [\n",
    "        ProcessingOutput(\n",
    "            output_name = \"train_data\",\n",
    "            source = \"/opt/ml/processing/output/train\",\n",
    "            destination = f\"{processing_output_data}/train\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = \"validation_data\",\n",
    "            source = \"/opt/ml/processing/output/validation\",\n",
    "            destination = f\"{processing_output_data}/validation\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = \"test_data\",\n",
    "            source = \"/opt/ml/processing/output/test\",\n",
    "            destination = f\"{processing_output_data}/test\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    processing_step = ProcessingStep(\n",
    "        \"Preprocessing\",\n",
    "        processor = preprocessing_processor,\n",
    "        job_name = processing_job_placeholder,\n",
    "        inputs = inputs,\n",
    "        outputs = outputs,\n",
    "        container_arguments = [\"--data-file\", data_file],\n",
    "        container_entrypoint = [\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"],\n",
    "        experiment_config = {\n",
    "            \"TrialName\": trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Processing\",\n",
    "        }\n",
    "    )    \n",
    "\n",
    "    return processing_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define experiment related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(experiment_name):\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name = experiment_name, \n",
    "        description = \"Classification of target direct marketing\", \n",
    "        sagemaker_boto_client = sm\n",
    "    )\n",
    "    return experiment\n",
    "\n",
    "def create_trial(experiment_name, trial_name):\n",
    "    trial = Trial.create(\n",
    "        trial_name = trial_name, \n",
    "        experiment_name = experiment_name,\n",
    "        sagemaker_boto_client = sm,\n",
    "    )\n",
    "    return trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Hyperparameter Tuning Step\n",
    "\n",
    "Define HPO Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_hpo_step(\n",
    "    tuning_job_name_placeholder, \n",
    "    image_uri, \n",
    "    bucket_name, \n",
    "    sagemaker_execution_role,\n",
    "    ml_instance_count = 1,\n",
    "    ml_instance_type = 'ml.m5.xlarge',\n",
    "    objective_metric_name = 'validation:auc'\n",
    "):\n",
    "    tuning_output_path = f's3://{bucket_name}/tuning/output'\n",
    "\n",
    "    tuning_estimator = sagemaker.estimator.Estimator(\n",
    "        image_uri,\n",
    "        sagemaker_execution_role, \n",
    "        instance_count = ml_instance_count, \n",
    "        instance_type = ml_instance_type,\n",
    "        output_path = tuning_output_path,\n",
    "        sagemaker_session = sagemaker_session\n",
    "    )    \n",
    "    hpo = dict(\n",
    "        max_depth = 5,\n",
    "        eta = 0.2,\n",
    "        gamma = 4,\n",
    "        min_child_weight = 6,\n",
    "        subsample = 0.8,\n",
    "        silent = 0,\n",
    "        objective = 'binary:logistic',\n",
    "        num_round = 100\n",
    "    ) \n",
    "    tuning_estimator.set_hyperparameters(**hpo)\n",
    "    \n",
    "    hyperparameter_ranges = {\n",
    "        'eta': ContinuousParameter(0, 1),\n",
    "        'min_child_weight': ContinuousParameter(1, 10),\n",
    "        'alpha': ContinuousParameter(0, 2),\n",
    "        'max_depth': IntegerParameter(1, 10)\n",
    "    }\n",
    "    hpo_tuner = HyperparameterTuner(\n",
    "        tuning_estimator,\n",
    "        objective_metric_name,\n",
    "        hyperparameter_ranges,\n",
    "        max_jobs = 20,\n",
    "        max_parallel_jobs = 3\n",
    "    )\n",
    "\n",
    "    processing_output_data = f\"s3://{bucket_name}/preprocessing/output\"\n",
    "    s3_input_train = TrainingInput(s3_data = f'{processing_output_data}/train', content_type = 'csv')\n",
    "    s3_input_validation = TrainingInput(s3_data = f'{processing_output_data}/validation', content_type = 'csv')\n",
    "    hpo_data = dict(\n",
    "        train = s3_input_train,\n",
    "        validation = s3_input_validation\n",
    "    )\n",
    "\n",
    "    # as long as HPO is selected, wait for completion.\n",
    "    tuning_step = TuningStep(\n",
    "        \"HPO Step\",\n",
    "        tuner = hpo_tuner,\n",
    "        job_name = tuning_job_name_placeholder,\n",
    "        data = hpo_data,\n",
    "        wait_for_completion = True\n",
    "    )\n",
    "\n",
    "    return tuning_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `query_hpo_job` lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function\n",
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_hpo_job.zip'\n",
    "lambda_source_code = './code/query_hpo_job.py'\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/code\",\n",
    "                  sagemaker_session = sagemaker_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "lambda_function_query_hpo_job = 'query_hpo_job'\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName = lambda_function_query_hpo_job,\n",
    "    Runtime = 'python3.7',\n",
    "    Role = sagemaker_execution_role,\n",
    "    Handler = 'query_hpo_job.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket_name,\n",
    "        'S3Key': f'code/{zip_name}'\n",
    "    },\n",
    "    Description='Queries SageMaker HPO Job.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_query_hpo_job_step(lambda_function_name_query_hpo_job_placeholder):\n",
    "    query_hpo_job_lambda_step = LambdaStep(\n",
    "        'Query HPO Job',\n",
    "        parameters = {  \n",
    "            \"FunctionName\": lambda_function_name_query_hpo_job_placeholder,\n",
    "            'Payload':{\n",
    "                \"HpoJobName.$\": \"$$.Execution.Input['TuningJobName']\"\n",
    "            }\n",
    "        }\n",
    "    )   \n",
    "    return query_hpo_job_lambda_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SNS Topic and Complete Subscription. Please acknolwedge the topic subscription on the email id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns = boto3.client('sns')\n",
    "response = sns.create_topic(Name = topic_name)\n",
    "\n",
    "topic_arn = response['TopicArn']\n",
    "\n",
    "response = sns.subscribe(\n",
    "    TopicArn = topic_arn,\n",
    "    Protocol = 'email',\n",
    "    Endpoint = email_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hpo_job_sns_notification_step(\n",
    "    topic_arn,\n",
    "    query_hpo_job_lambda_step\n",
    "):\n",
    "    hpo_job_sns_step = SnsPublishStep(\n",
    "        state_id = 'SNS Notification - HPO Job',\n",
    "        parameters = {\n",
    "            'TopicArn': topic_arn,\n",
    "            'Message': query_hpo_job_lambda_step.output()['Payload']['bestTrainingJob']\n",
    "        }\n",
    "    )    \n",
    "    return hpo_job_sns_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Training Step\n",
    "\n",
    "Define Model Training Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_step(\n",
    "    training_job_name_placeholer, \n",
    "    image_uri, \n",
    "    bucket_name, \n",
    "    experiment_name,\n",
    "    trial_name,\n",
    "    role, \n",
    "    ml_instance_count = 1,\n",
    "    ml_instance_type = 'ml.m5.xlarge',\n",
    "):\n",
    "    training_output_path = f's3://{bucket_name}/training/output'\n",
    "    training_estimator = sagemaker.estimator.Estimator(\n",
    "        image_uri,\n",
    "        role, \n",
    "        instance_count = ml_instance_count, \n",
    "        instance_type = ml_instance_type,\n",
    "        output_path = training_output_path,\n",
    "        sagemaker_session = sagemaker_session\n",
    "    )\n",
    "        \n",
    "    hpo = dict(\n",
    "        max_depth = 5,\n",
    "        eta = 0.2,\n",
    "        gamma = 4,\n",
    "        min_child_weight = 6,\n",
    "        subsample = 0.8,\n",
    "        silent = 0,\n",
    "        objective = 'binary:logistic',\n",
    "        num_round = 100\n",
    "    )\n",
    "    training_estimator.set_hyperparameters(**hpo) \n",
    "    \n",
    "    processing_output_data = f\"s3://{bucket_name}/preprocessing/output\"\n",
    "    s3_input_train = sagemaker.inputs.TrainingInput(s3_data=f'{processing_output_data}/train', content_type='csv')\n",
    "    s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=f'{processing_output_data}/validation', content_type='csv')\n",
    "\n",
    "    training_data = dict(\n",
    "        train = s3_input_train,\n",
    "        validation = s3_input_validation\n",
    "    )\n",
    "\n",
    "    training_step = TrainingStep(\n",
    "        \"Model Training\",\n",
    "        estimator = training_estimator,\n",
    "        data = training_data,\n",
    "        job_name = training_job_name_placeholer,\n",
    "        wait_for_completion = True,\n",
    "        experiment_config = {\n",
    "            \"TrialName\": trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "    )    \n",
    "    return training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best training job hyperparameters\n",
    "{'_tuning_objective_metric': 'validation:auc',\n",
    " 'alpha': '1.9167548939755026',\n",
    " 'eta': '0.2513705646042541',\n",
    " 'gamma': '4',\n",
    " 'max_depth': '4',\n",
    " 'min_child_weight': '2.561240034842159',\n",
    " 'num_round': '100',\n",
    " 'objective': 'binary:logistic',\n",
    " 'silent': '0',\n",
    " 'subsample': '0.8'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Step\n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_step(\n",
    "    model_name_placeholder, \n",
    "    training_step\n",
    "):\n",
    "    model_step = ModelStep(\n",
    "        \"Save Model\",\n",
    "        model = training_step.get_expected_model(),\n",
    "        model_name = model_name_placeholder\n",
    "    )\n",
    "    return model_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as HPO and Model Training are False and existing model uri is provided, we can deploy existing model on Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_existing_model_step(\n",
    "    model_name_placeholder, \n",
    "    existing_model_name,\n",
    "    image_uri, \n",
    "    existing_model_uri,\n",
    "    sagemaker_execution_role\n",
    "):\n",
    "    # for deploying existing model\n",
    "    existing_model = Model(\n",
    "        model_data = existing_model_uri,\n",
    "        image_uri = image_uri,\n",
    "        role = sagemaker_execution_role,\n",
    "        name = existing_model_name\n",
    "    )\n",
    "    existing_model_step = ModelStep(\n",
    "        \"Using Existing Model\",\n",
    "        model = existing_model,\n",
    "        model_name = model_name_placeholder\n",
    "    )\n",
    "    return existing_model_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Configuration Step\n",
    "\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "Define EndpointConfig Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint_configurgation_step(\n",
    "    endpoint_config_name_placeholder, \n",
    "    model_name_placeholder, \n",
    "    ml_instance_count = 1,\n",
    "    ml_instance_type = 'ml.m5.xlarge'\n",
    "):\n",
    "    endpoint_config_step = EndpointConfigStep(\n",
    "        \"Create Endpoint Config\",\n",
    "        endpoint_config_name = endpoint_config_name_placeholder,\n",
    "        model_name = model_name_placeholder,\n",
    "        initial_instance_count = ml_instance_count,\n",
    "        instance_type = ml_instance_type\n",
    "    )\n",
    "    return endpoint_config_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda function to check Endpoint Existed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_endpoint_existence.zip'\n",
    "lambda_source_code = './code/query_endpoint_existence.py'\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/code\",\n",
    "                  sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "lambda_function_query_endpoint = 'query_endpoint'\n",
    "try: \n",
    "    response = lambda_client.create_function(\n",
    "        FunctionName = lambda_function_query_endpoint,\n",
    "        Runtime = 'python3.7',\n",
    "        Role = sagemaker_execution_role,\n",
    "        Handler = 'query_endpoint_existence.lambda_handler',\n",
    "        Code={\n",
    "            'S3Bucket': bucket_name,\n",
    "            'S3Key': f'code/{zip_name}'\n",
    "        },\n",
    "        Description='Queries a SageMaker Endpoint existence.',\n",
    "        Timeout=15,\n",
    "        MemorySize=128\n",
    "    )\n",
    "except:\n",
    "    print(\"Function already exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `query_endpoint` lambda step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_query_endpoint_step(lambda_function_name_query_endpoint_placeholder):\n",
    "    query_endpoint_lambda_step = LambdaStep(\n",
    "        'Query Endpoint Info',\n",
    "        parameters = {  \n",
    "            \"FunctionName\": lambda_function_name_query_endpoint_placeholder,\n",
    "            'Payload':{\n",
    "                \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return query_endpoint_lambda_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Step\n",
    "\n",
    "In the following cells, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is sucessful.\n",
    "\n",
    "Define Endpoint Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint_step(endpoint_name_placeholder, endpoint_config_name_placeholder, update = False):\n",
    "    endpoint_step = EndpointStep(\n",
    "        \"Update Endpoint\" if update else \"Create Endpoint\",\n",
    "        endpoint_name = endpoint_name_placeholder,\n",
    "        endpoint_config_name = endpoint_config_name_placeholder,\n",
    "        update = update\n",
    "    )\n",
    "    return endpoint_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Endpoint Deployment Status Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_endpoint_deployment_lambda_step(lambda_function_name_query_endpoint_placeholder):\n",
    "    query_endpoint_deployment_lambda_step = LambdaStep(\n",
    "        'Query Endpoint Deployment Status',\n",
    "        parameters = {  \n",
    "            \"FunctionName\": lambda_function_name_query_endpoint_placeholder,\n",
    "            'Payload':{\n",
    "                \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return query_endpoint_deployment_lambda_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Choices to check conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice step is to check whether the endpoint is `InService` so that we can update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_check_endpoint_status_choice_step(\n",
    "    query_endpoint_lambda_step,\n",
    "    endpoint_update_step\n",
    "):\n",
    "    check_endpoint_status_step = Choice('Endpoint is ready for deployment?')\n",
    "\n",
    "    endpoint_in_service_rule = ChoiceRule.StringEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_status'], value = 'InService')\n",
    "    check_endpoint_status_step.add_choice(rule = endpoint_in_service_rule, next_step = endpoint_update_step)\n",
    "    \n",
    "    # in case endpoint is in 'failed' state, we allow it to update so as to trigger exception.\n",
    "    endpoint_failed_rule = ChoiceRule.StringEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_status'], value = 'Failed')\n",
    "    check_endpoint_status_step.add_choice(rule = endpoint_failed_rule, next_step = endpoint_update_step)\n",
    "\n",
    "    wait_step = Wait(state_id = f\"Wait until Endpoint is ready\", seconds = 20)\n",
    "    wait_step.next(query_endpoint_lambda_step)\n",
    "    check_endpoint_status_step.default_choice(next_step = wait_step)  \n",
    "\n",
    "    return check_endpoint_status_step  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice step is to check whether the endpoint exists or not. If Not, we create it, otherwise, check the endpoint status before updating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_check_endpoint_existence_choice_step(\n",
    "    query_endpoint_lambda_step,\n",
    "    check_endpoint_status_step,\n",
    "    endpoint_creation_step\n",
    "):\n",
    "    check_endpoint_existence_step = Choice('Endpoint Existed?')\n",
    "\n",
    "    endpoint_existed_rule = ChoiceRule.BooleanEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_existed'], value = True)\n",
    "    check_endpoint_existence_step.add_choice(rule = endpoint_existed_rule, next_step = check_endpoint_status_step)\n",
    "\n",
    "    check_endpoint_existence_step.default_choice(next_step = endpoint_creation_step)\n",
    "    return check_endpoint_existence_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, once we create/update an endpoint, we wait until the status turns `InService`.\n",
    "we refer to API [Choice](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/states.html?highlight=choice#stepfunctions.steps.states.Choice) to check the status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_check_endpoint_is_deploying_choice_step(\n",
    "    query_endpoint_deployment_lambda_step,\n",
    "    success_notification_step\n",
    "):\n",
    "    # check endpoint readiness\n",
    "    deployed_endpoint_updating_step = Choice('Endpoint is deploying?')\n",
    "\n",
    "    wait_deployment_step = Wait(state_id = \"Wait Until Deployment is Completed...\", seconds = 20)\n",
    "    wait_deployment_step.next(query_endpoint_deployment_lambda_step)\n",
    "\n",
    "    deployed_endpoint_updating_rule = ChoiceRule.StringEquals(variable = query_endpoint_deployment_lambda_step.output()['Payload']['endpoint_status'], value = 'InService')\n",
    "    deployed_endpoint_updating_step.add_choice(rule = deployed_endpoint_updating_rule, next_step = success_notification_step)\n",
    "    \n",
    "    deployed_endpoint_updating_step.default_choice(next_step = wait_deployment_step)\n",
    "\n",
    "    return deployed_endpoint_updating_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice step is to check whether we process HPO or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_to_do_hpo_choice_step(\n",
    "    tuning_path,\n",
    "    training_choice\n",
    "):\n",
    "    to_do_hpo_choice = Choice(\"To Do HPO?\")\n",
    "\n",
    "    to_do_hpo_choice.add_choice(\n",
    "        rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = True),\n",
    "        next_step = tuning_path                 \n",
    "    )\n",
    "    to_do_hpo_choice.add_choice(\n",
    "        rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = False),\n",
    "        next_step = training_choice\n",
    "    )\n",
    "    return to_do_hpo_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice step is to check whether we do model training or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_to_do_training_choice_step(\n",
    "    training_path,\n",
    "    deploy_existing_model_path\n",
    "):\n",
    "    to_do_training_choice = Choice(\"To Do Model Training?\")\n",
    "\n",
    "    to_do_training_choice.add_choice(\n",
    "        rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = True),\n",
    "        next_step = training_path\n",
    "    )\n",
    "    to_do_training_choice.add_choice(\n",
    "        rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = False),\n",
    "        next_step = deploy_existing_model_path\n",
    "    )\n",
    "    return to_do_training_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define failure/success step notification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_failure_notification_step(\n",
    "    topic_arn\n",
    "):\n",
    "    failure_sns_step = SnsPublishStep(\n",
    "        state_id = 'SNS Notification - Pipeline Failure',\n",
    "        parameters = {\n",
    "            'TopicArn': topic_arn,\n",
    "            'Message.$': \"$\",\n",
    "            'Subject': '[ML Pipeline] Execution failed...'\n",
    "        }\n",
    "    )    \n",
    "    return failure_sns_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_success_notification_step(topic_arn):\n",
    "    success_sns_step = SnsPublishStep(\n",
    "        state_id = 'SNS Notification - Pipeline Succeeded',\n",
    "        parameters = {\n",
    "            'TopicArn': topic_arn,\n",
    "            'Message.$': \"$$.Execution.Id\",\n",
    "            'Subject': '[ML Pipeline] Execution completed successfully!'\n",
    "        }\n",
    "    )    \n",
    "    return success_sns_step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to check state machine information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_machine_arn(workflow_name, region, account_id):\n",
    "    return f\"arn:aws:states:{region}:{account_id}:stateMachine:{workflow_name}\"\n",
    "\n",
    "def is_workflow_existed(workflow_role_arn):\n",
    "    try:\n",
    "        sfn_client = boto3.client('stepfunctions')\n",
    "        response = sfn_client.describe_state_machine(\n",
    "            stateMachineArn = workflow_role_arn\n",
    "        )\n",
    "        return True\n",
    "    except: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function contains workflow construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workflow(\n",
    "    bucket_name, \n",
    "    data_file,\n",
    "    topic_name,\n",
    "    experiment_name,\n",
    "    workflow_name,\n",
    "    region, \n",
    "    account_id,\n",
    "    workflow_execution_role,\n",
    "    sagemaker_execution_role\n",
    "):\n",
    "    suffix = datetime.now().strftime(\"%y%m%d-%H%M\")\n",
    "\n",
    "    # Workflow Execution parameters\n",
    "    execution_input = ExecutionInput(\n",
    "        schema = {\n",
    "            \"PreprocessingJobName\": str,\n",
    "            \"ToDoHPO\": bool,\n",
    "            \"ToDoTraining\": bool,\n",
    "            \"TrainingJobName\": str,\n",
    "            \"TuningJobName\": str,\n",
    "            \"ModelName\": str,\n",
    "            \"EndpointConfigName\": str,\n",
    "            \"EndpointName\": str,\n",
    "            \"LambdaFunctionNameOfQueryEndpoint\": str,\n",
    "            \"LambdaFunctionNameOfQueryHpoJob\": str\n",
    "        }\n",
    "    )\n",
    "    image_uri = sagemaker.image_uris.retrieve(region = region, framework='xgboost', version='latest')\n",
    "\n",
    "    # create the steps\n",
    "    trial = create_trial(experiment_name, f\"xgb-processing-job-{suffix}\")\n",
    "    input_code_uri = upload_preprocess_code(bucket_name)\n",
    "    processing_step = create_preprocessing_step(\n",
    "        execution_input[\"PreprocessingJobName\"], \n",
    "        input_code_uri, \n",
    "        bucket_name,\n",
    "        data_file, \n",
    "        experiment_name,\n",
    "        trial.trial_name,\n",
    "        sagemaker_execution_role\n",
    "    )\n",
    "    \n",
    "    tuning_step = create_hpo_step(execution_input[\"TuningJobName\"], image_uri, bucket_name, sagemaker_execution_role)\n",
    "    query_hpo_job_lambda_step = create_lambda_query_hpo_job_step(execution_input['LambdaFunctionNameOfQueryHpoJob'])\n",
    "    topic_arn = f\"arn:aws:sns:{region}:{account_id}:{topic_name}\"\n",
    "    hpo_job_sns_notification_step = create_hpo_job_sns_notification_step(topic_arn, query_hpo_job_lambda_step)\n",
    "    training_trial = create_trial(experiment_name, f\"xgb-training-job-{suffix}\")\n",
    "    training_step = create_training_step(execution_input[\"TrainingJobName\"], image_uri, bucket_name, experiment_name, training_trial.trial_name, sagemaker_execution_role)\n",
    "    model_step = create_model_step(execution_input[\"ModelName\"], training_step)\n",
    "    existing_model_uri = f\"s3://{bucket_name}/{S3_KEY_TRAINED_MODEL}\"\n",
    "    existing_model_step = create_existing_model_step(execution_input[\"ModelName\"], f\"dm-model-{suffix}\", image_uri, existing_model_uri, sagemaker_execution_role)\n",
    "    query_endpoint_lambda_step = create_lambda_query_endpoint_step(execution_input['LambdaFunctionNameOfQueryEndpoint'])\n",
    "    endpoint_config_step = create_endpoint_configurgation_step(\n",
    "        execution_input[\"EndpointConfigName\"], \n",
    "        execution_input[\"ModelName\"]\n",
    "    )\n",
    "    endpoint_creation_step = create_endpoint_step(execution_input[\"EndpointName\"], execution_input[\"EndpointConfigName\"], False)\n",
    "    endpoint_update_step = create_endpoint_step(execution_input[\"EndpointName\"], execution_input[\"EndpointConfigName\"], True)\n",
    "    query_endpoint_deployment_lambda_step = create_query_endpoint_deployment_lambda_step(execution_input['LambdaFunctionNameOfQueryEndpoint'])\n",
    "\n",
    "    # create the choice steps\n",
    "    check_endpoint_status_choice_step = create_check_endpoint_status_choice_step(query_endpoint_lambda_step, endpoint_update_step)\n",
    "    check_endpoint_existence_choice_step = create_check_endpoint_existence_choice_step(\n",
    "        query_endpoint_lambda_step,\n",
    "        check_endpoint_status_choice_step,\n",
    "        endpoint_creation_step\n",
    "    )\n",
    "    success_notification_step = create_success_notification_step(topic_arn)\n",
    "    check_endpoint_is_deploying_choice_step = create_check_endpoint_is_deploying_choice_step(\n",
    "        query_endpoint_deployment_lambda_step,\n",
    "        success_notification_step\n",
    "    )\n",
    "\n",
    "    query_endpoint_deployment_lambda_step.next(check_endpoint_is_deploying_choice_step)\n",
    "    endpoint_creation_step.next(query_endpoint_deployment_lambda_step)\n",
    "    endpoint_update_step.next(query_endpoint_deployment_lambda_step)\n",
    "    \n",
    "    training_path = Chain(\n",
    "        [\n",
    "            training_step, \n",
    "            model_step, \n",
    "            endpoint_config_step, \n",
    "            query_endpoint_lambda_step, \n",
    "            check_endpoint_existence_choice_step\n",
    "        ]\n",
    "    )\n",
    "    deploy_existing_model_path = Chain(\n",
    "        [\n",
    "            existing_model_step, \n",
    "            endpoint_config_step, \n",
    "            query_endpoint_lambda_step, \n",
    "            check_endpoint_existence_choice_step\n",
    "        ]\n",
    "    )\n",
    "    tuning_path = Chain([tuning_step, query_hpo_job_lambda_step, hpo_job_sns_notification_step])\n",
    "\n",
    "    to_do_training_choice_step = create_to_do_training_choice_step(training_path, deploy_existing_model_path)\n",
    "    to_do_hpo_choice_step = create_to_do_hpo_choice_step(tuning_path, to_do_training_choice_step)\n",
    "\n",
    "    # catch execution exception\n",
    "    failed_state_sagemaker_pipeline_step_failure = Fail(\n",
    "        \"ML Workflow Failed\", cause = \"SageMakerPipelineStepFailed\"\n",
    "    )\n",
    "    failure_notification_step = create_failure_notification_step(topic_arn)\n",
    "    \n",
    "    catch_state_processing = Catch(\n",
    "        error_equals = [\"States.TaskFailed\"],\n",
    "        next_step = Chain([failure_notification_step, failed_state_sagemaker_pipeline_step_failure])\n",
    "    )\n",
    "    processing_step.add_catch(catch_state_processing)\n",
    "    tuning_step.add_catch(catch_state_processing)\n",
    "    training_step.add_catch(catch_state_processing)\n",
    "    model_step.add_catch(catch_state_processing)\n",
    "    endpoint_config_step.add_catch(catch_state_processing)\n",
    "    endpoint_creation_step.add_catch(catch_state_processing)\n",
    "    endpoint_update_step.add_catch(catch_state_processing)\n",
    "    existing_model_step.add_catch(catch_state_processing)\n",
    "    \n",
    "    workflow_graph = Chain([processing_step, to_do_hpo_choice_step])\n",
    "#     workflow_graph = Chain([to_do_hpo_choice_step])\n",
    "\n",
    "    # Create Workflow\n",
    "    workflow_arn = get_state_machine_arn(workflow_name, region, account_id)\n",
    "    workflow_existed = is_workflow_existed(workflow_arn)\n",
    "    if workflow_existed:\n",
    "        # To update SFN workflow, need to do 'attach' & 'update' together.\n",
    "        workflow = Workflow.attach(state_machine_arn = workflow_arn)\n",
    "        workflow.update(definition = workflow_graph, role = workflow_execution_role) \n",
    "        # Wait for 10s so that the update is completed before executing workflow\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        workflow = Workflow(\n",
    "            name = workflow_name,\n",
    "            definition = workflow_graph,\n",
    "            role = workflow_execution_role\n",
    "        )\n",
    "        workflow.create()\n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow creation and execution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "experiment = create_experiment(f\"xgboost-target-direct-marketing-{suffix}\")\n",
    "data_file = \"bank-additional-full.csv\" \n",
    "workflow_name = WORKFLOW_NAME\n",
    "workflow_execution_role = WORKFLOW_EXECUTION_ROLE\n",
    "\n",
    "# bucket_name is created in ml_pipeline_dependencies.py, which is imported at the beginning.\n",
    "workflow = create_workflow(\n",
    "    bucket_name, \n",
    "    data_file,\n",
    "    topic_name,\n",
    "    experiment.experiment_name,\n",
    "    workflow_name, \n",
    "    region, \n",
    "    account_id,\n",
    "    workflow_execution_role,\n",
    "    sagemaker_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute workflow- for blue-model creation\n",
    "suffix = datetime.now().strftime(\"%y%m%d-%H%M\")\n",
    "\n",
    "# execution input parameter values\n",
    "require_hpo = False\n",
    "require_model_training = False \n",
    "preprocessing_job_name = f\"dm-preprocessing-{suffix}\"\n",
    "tuning_job_name = f\"dm-tuning-{suffix}\"\n",
    "training_job_name = f\"dm-training-{suffix}\"\n",
    "model_job_name = f\"dm-model-{suffix}\"\n",
    "endpoint_config_name = f\"dm-endpoint-config-{suffix}\"\n",
    "endpoint_job_name = \"direct-marketing-endpoint-manual\"\n",
    "lambda_function_query_endpoint = 'query_endpoint'\n",
    "lambda_function_query_hpo_job = 'query_hpo_job'\n",
    "\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": require_hpo,\n",
    "        \"ToDoTraining\": require_model_training,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointConfigName\": endpoint_config_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": lambda_function_query_endpoint,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": lambda_function_query_hpo_job\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell multiple times to observe the workflow execution progress. Please note that the execution may take 15-20mins with using existing model for batch transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress(portrait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after the workflow is done, run the following cell to generate another job for endpoint update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute workflow- for green-model creation\n",
    "\n",
    "suffix = datetime.now().strftime(\"%y%m%d-%H%M\")\n",
    "\n",
    "# execution input parameter values\n",
    "require_hpo = False\n",
    "require_model_training = False \n",
    "preprocessing_job_name = f\"dm-preprocessing-{suffix}\"\n",
    "tuning_job_name = f\"dm-tuning-{suffix}\"\n",
    "training_job_name = f\"dm-training-{suffix}\"\n",
    "model_job_name = f\"dm-model-{suffix}\"\n",
    "endpoint_config_name = f\"dm-endpoint-config-{suffix}\"\n",
    "endpoint_job_name = \"direct-marketing-endpoint-manual\"\n",
    "lambda_function_query_endpoint = 'query_endpoint'\n",
    "lambda_function_query_hpo_job = 'query_hpo_job'\n",
    "\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": require_hpo,\n",
    "        \"ToDoTraining\": require_model_training,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointConfigName\": endpoint_config_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": lambda_function_query_endpoint,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": lambda_function_query_hpo_job\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean-up\n",
    "\n",
    "If you are done with this notebook, please run the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.delete_endpoint(EndpointName=endpoint_job_name )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
