{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeting Direct Marketing Model Training ML Pipeline [manual]\n",
    "\n",
    "---\n",
    "\n",
    "Once you are familiar with using Amazon SageMaker built-in algorithm - [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to do [Targetting Direct Marketing model traing](./01_xgboost_direct_marketing_sagemaker.ipynb), we are going to build a ML Pipeline to automate the workflow with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). \n",
    "\n",
    "In the design:\n",
    "* There is preprocessing job to do data integration\n",
    "  * A table is created in Amazon Athena to query data on open air quality data. Visit [Open AQ](https://openaq.org/) for detail.\n",
    "  * A query to Amazon Athena to collect Sydney, Australia air quality data.\n",
    "  * Data cleansing and feature engineering\n",
    "  * Train and test data set are separated; we keep last 30 days' data as test set.\n",
    "  * Batch Transform test data is constructed based on the latest 100 record in test set. \n",
    "* Hyperparameters optimization is optional\n",
    "  * In pipeline, we will leave hyperparameter optimziation alone without doing batch transform.\n",
    "* Model training with tuned hyperparameters\n",
    "  * For example, you may collect the hyperparameters from HPO jobs with the best candidate.\n",
    "* Batch transform job is triggerred to forecast air quality.\n",
    "  * In the example, we do the batch inference for the latest 100 records in our test data set. \n",
    "\n",
    "In the notebook, we are going to demo how to create the workflow step by step and process till model training. Below is the related Step Functions workflow mapping to the ML pipeline with no HPO and using an trained model:\n",
    "\n",
    "![Direct Marketing](./img/air_quality_forecasting_ml_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Creation\n",
    "---\n",
    "To create ML pipeline, we will use Step Functions Data Science SDK v2.0.0rc1, which is compatible with SageMaker SDK 2.x.\n",
    "\n",
    "We will cover pipeline creation at below:\n",
    "* Environment initialization\n",
    "* Build Docker image for SageMaker Processing\n",
    "* Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" # 2.0.0\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0rc1\"\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "\n",
    "import stepfunctions\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.steps.sagemaker import *\n",
    "from stepfunctions.steps.states import *\n",
    "from stepfunctions.steps.compute import *\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.steps import *\n",
    "from IPython.display import display, HTML, Javascript\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "\n",
    "session = boto3.Session()\n",
    "sm = session.client('sagemaker')\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm/manual_pipeline'\n",
    "account_id = session.client('sts').get_caller_identity().get('Account')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the workflow execution role. For the role arn, please refer to the output tab of the CloudFormation stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssm = boto3.client('ssm')\n",
    "# response = ssm.get_parameter(Name = \"/directmarketing/ml_pipeline/workflow_execution_role\")\n",
    "# WORKFLOW_EXECUTION_ROLE = response['Parameter']['Value']\n",
    "\n",
    "WORKFLOW_EXECUTION_ROLE = \"arn:aws:iam::593380422482:role/StepFunctionsWorkflowExecutionRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow execution IAM service role: arn:aws:iam::593380422482:role/StepFunctionsWorkflowExecutionRole\n"
     ]
    }
   ],
   "source": [
    "if not WORKFLOW_EXECUTION_ROLE:\n",
    "    raise Exception(\"ML Pipeline Parameters in System Manager is not setup properly. Please check whether the ml-pipeline stack has been created or not.\")\n",
    "else:\n",
    "    print(f\"Workflow execution IAM service role: {WORKFLOW_EXECUTION_ROLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTING_MODEL_URI = \"s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/output/xgboost-201120-0017-007-fc507e21/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-23 23:58:29--  https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
      "Resolving sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)... 52.218.234.233\n",
      "Connecting to sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)|52.218.234.233|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 432828 (423K) [application/zip]\n",
      "Saving to: ‘bank-additional.zip.6’\n",
      "\n",
      "bank-additional.zip 100%[===================>] 422.68K   759KB/s    in 0.6s    \n",
      "\n",
      "2020-11-23 23:58:30 (759 KB/s) - ‘bank-additional.zip.6’ saved [432828/432828]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./bank-additional.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)\n",
    "\n",
    "---\n",
    "\n",
    "#### Create Processing Step for data preprocessing\n",
    "\n",
    "We will now create the [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) that will launch a SageMaker Processing Job.\n",
    "\n",
    "In the processing job script `./pipeline/ml_pipeline_preprocessing.py`, the actions will be done:\n",
    "\n",
    "* Feature engineering on the dataset\n",
    "* Split training and test data \n",
    "* Store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_SCRIPT_LOCATION = \"./pipeline/preprocessing.py\"\n",
    "input_code_uri = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket = bucket_name,\n",
    "    key_prefix = f\"{prefix}/preprocessing/code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SKLearnProcessor` class lets you run a command inside the container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "preprocessing_processor = SKLearnProcessor(\n",
    "    framework_version='0.20.0',\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    max_runtime_in_seconds = 1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 locations of preprocessing output with training, test & all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = f\"s3://{bucket_name}/{prefix}/preprocessing/output\"\n",
    "processing_input_path = f's3://{bucket_name}/{prefix}/preprocessing/input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_file = './bank-additional/bank-additional-full.csv'\n",
    "sagemaker.s3.S3Uploader.upload(local_data_file, processing_input_path, sagemaker_session = sagemaker_session)\n",
    "input_data = f'{processing_input_path}/bank-additional-full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/manual_pipeline/preprocessing/input/bank-additional-full.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-23 23:58:32    5146674 bank-additional-full.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/manual_pipeline/preprocessing/input/bank-additional-full.csv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will use ScriptProcessor as defined in previous steps along with the inputs and outputs objects that are defined in the below steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name = \"code\",\n",
    "        source = input_code_uri,\n",
    "        destination = \"/opt/ml/processing/input/code\"\n",
    "    ),\n",
    "    ProcessingInput(\n",
    "        input_name = \"input_data\",\n",
    "        source = input_data,\n",
    "        destination='/opt/ml/processing/input'\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name = \"train_data\",\n",
    "        source = \"/opt/ml/processing/output/train\",\n",
    "        destination = f\"{output_data}/train\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name = \"validation_data\",\n",
    "        source = \"/opt/ml/processing/output/validation\",\n",
    "        destination = f\"{output_data}/validation\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name = \"test_data\",\n",
    "        source = \"/opt/ml/processing/output/test\",\n",
    "        destination = f\"{output_data}/test\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Execution parameters\n",
    "execution_input = ExecutionInput(\n",
    "    schema = {\n",
    "        \"PreprocessingJobName\": str,\n",
    "        \"ToDoHPO\": bool,\n",
    "        \"ToDoTraining\": bool,\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TuningJobName\": str,\n",
    "        \"ModelName\": str,\n",
    "        \"EndpointName\": str,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": str,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7faec5dd2048>,experiment_name='xgboost-target-direct-marketing-1606175913',description='Classification of target direct marketing',tags=None,experiment_arn='arn:aws:sagemaker:ap-southeast-2:593380422482:experiment/xgboost-target-direct-marketing-1606175913',response_metadata={'RequestId': '80e8b0c9-5ef5-484b-bd61-da7d1d043432', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '80e8b0c9-5ef5-484b-bd61-da7d1d043432', 'content-type': 'application/x-amz-json-1.1', 'content-length': '119', 'date': 'Mon, 23 Nov 2020 23:58:33 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "# Create Experiment\n",
    "experiment = Experiment.create(\n",
    "    experiment_name = f\"xgboost-target-direct-marketing-{int(time.time())}\", \n",
    "    description = \"Classification of target direct marketing\", \n",
    "    sagemaker_boto_client = sm)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = f\"xgb-processing-job-{int(time.time())}\"\n",
    "xgb_trial = Trial.create(\n",
    "    trial_name = trial_name, \n",
    "    experiment_name = experiment.experiment_name,\n",
    "    sagemaker_boto_client = sm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProcessingStep` queries open air quality data for Sydney Australia with Amazon Athena. Especially, we are using our bucket to store query result. In case you setup default workgroup in Amazon Athena, please ensure to uncheck ***Override client-side settings***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    \"DM Preprocessing Step\",\n",
    "    processor = preprocessing_processor,\n",
    "    job_name = execution_input[\"PreprocessingJobName\"],\n",
    "    inputs = inputs,\n",
    "    outputs = outputs,\n",
    "    container_arguments = [\"--data-file\", \"bank-additional-full.csv\"],\n",
    "    container_entrypoint = [\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"],\n",
    "    experiment_config = {\n",
    "        \"TrialName\": xgb_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Processing\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Hyperparameter Tuning Step\n",
    "\n",
    "Setup tuning step and use choice state to decide whether we should do HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "tuning_output_path = f's3://{bucket_name}/{prefix}/tuning/output'\n",
    "image_uri = sagemaker.image_uris.retrieve(region = region, framework='xgboost', version='latest')\n",
    "\n",
    "tuning_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role, \n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    output_path = tuning_output_path,\n",
    "    sagemaker_session = sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set static hyperparameters\n",
    "The static parameters are the ones we know to be the best based on previously run HPO jobs, as well as the non-tunable parameters like prediction length and time frequency that are set according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = dict(\n",
    "    max_depth = 5,\n",
    "    eta = 0.2,\n",
    "    gamma = 4,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.8,\n",
    "    silent = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    num_round = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set hyper-parameter ranges\n",
    "The hyperparameter ranges define the parameters we want the runer to search across.\n",
    "\n",
    "> Explore: Look in the [user guide](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html) for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0, 1),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2),\n",
    "    'max_depth': IntegerParameter(1, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create HPO tunning job step\n",
    "Once we have the HPO tuner defined, we can define the tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_estimator.set_hyperparameters(**hpo)\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "hpo_tuner = HyperparameterTuner(\n",
    "    tuning_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3\n",
    ")\n",
    "\n",
    "hpo_data = dict(\n",
    "    train = f\"{output_data}/train\",\n",
    "    test = f\"{output_data}/validation\"\n",
    ")\n",
    "# as long as HPO is selected, wait for completion.\n",
    "tuning_step = TuningStep(\n",
    "    \"HPO Step\",\n",
    "    tuner = hpo_tuner,\n",
    "    job_name = execution_input[\"TuningJobName\"],\n",
    "    data = hpo_data,\n",
    "    wait_for_completion = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function\n",
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_hpo_job.zip'\n",
    "lambda_source_code = './code/query_hpo_job.py'\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/{prefix}/code\",\n",
    "                  sagemaker_session = sagemaker_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "lambda_function_query_hpo_job = 'query_hpo_job'\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName = lambda_function_query_hpo_job,\n",
    "    Runtime = 'python3.7',\n",
    "    Role = role,\n",
    "    Handler = 'query_hpo_job.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket_name,\n",
    "        'S3Key': f'{prefix}/code/{zip_name}'\n",
    "    },\n",
    "    Description='Queries SageMaker HPO Job.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_hpo_job_lambda_step = LambdaStep(\n",
    "    'Query HPO Job',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryHpoJob'],\n",
    "        'Payload':{\n",
    "            \"HpoJobName.$\": \"$$.Execution.Input['TuningJobName']\"\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns = boto3.client('sns')\n",
    "topic_name = 'dm-model-training-notification-topic'\n",
    "response = sns.create_topic(Name = topic_name)\n",
    "\n",
    "topic_arn = response['TopicArn']\n",
    "email_id = 'tomlu@amazon.com'\n",
    "\n",
    "response = sns.subscribe(\n",
    "    TopicArn = topic_arn,\n",
    "    Protocol = 'email',\n",
    "    Endpoint = email_id,\n",
    "    ReturnSubscriptionArn = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_job_sns_step = SnsPublishStep(\n",
    "    state_id = 'SNS Notification - HPO Job',\n",
    "    parameters = {\n",
    "        'TopicArn': topic_arn,\n",
    "        'Message': query_hpo_job_lambda_step.output()['Payload']['bestTrainingJob']\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNS Notification - HPO Job SnsPublishStep(parameters={'TopicArn': 'arn:aws:sns:ap-southeast-2:593380422482:dm-model-training-notification-topic', 'Message': <stepfunctions.inputs.placeholders.StepInput object at 0x7faebe5b2080>}, resource='arn:aws:states:::sns:publish', type='Task')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_step.next(query_hpo_job_lambda_step)\n",
    "query_hpo_job_lambda_step.next(hpo_job_sns_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Training Step\n",
    "\n",
    "We create a DeepAR instance, which we will use to run a training job. This will be used to create a TrainingStep for the workflow.\n",
    "\n",
    "##### Setup the training job step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_path = f's3://{bucket_name}/{prefix}/training/output'\n",
    "training_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role, \n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    output_path = training_output_path,\n",
    "    sagemaker_session = sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tuning_objective_metric': 'validation:auc',\n",
       " 'alpha': '1.9167548939755026',\n",
       " 'eta': '0.2513705646042541',\n",
       " 'gamma': '4',\n",
       " 'max_depth': '4',\n",
       " 'min_child_weight': '2.561240034842159',\n",
       " 'num_round': '100',\n",
       " 'objective': 'binary:logistic',\n",
       " 'silent': '0',\n",
       " 'subsample': '0.8'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'_tuning_objective_metric': 'validation:auc',\n",
    " 'alpha': '1.9167548939755026',\n",
    " 'eta': '0.2513705646042541',\n",
    " 'gamma': '4',\n",
    " 'max_depth': '4',\n",
    " 'min_child_weight': '2.561240034842159',\n",
    " 'num_round': '100',\n",
    " 'objective': 'binary:logistic',\n",
    " 'silent': '0',\n",
    " 'subsample': '0.8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyper parameters for tuning\n",
    "hpo = dict(\n",
    "    max_depth = 5,\n",
    "    eta = 0.2,\n",
    "    gamma = 4,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.8,\n",
    "    silent = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    num_round = 100\n",
    ")\n",
    "training_estimator.set_hyperparameters(**hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/train', content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/validation', content_type='csv')\n",
    "\n",
    "training_data = dict(\n",
    "    train = s3_input_train,\n",
    "    validation = s3_input_validation\n",
    ")\n",
    "\n",
    "trial_name = f\"xgb-training-job-{int(time.time())}\"\n",
    "xgb_trial = Trial.create(\n",
    "    trial_name = trial_name, \n",
    "    experiment_name = experiment.experiment_name,\n",
    "    sagemaker_boto_client = sm,\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    \"Training Step\",\n",
    "    estimator = training_estimator,\n",
    "    data = training_data,\n",
    "    job_name = execution_input[\"TrainingJobName\"],\n",
    "    wait_for_completion = True,\n",
    "    experiment_config = {\n",
    "        \"TrialName\": xgb_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Step\n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    }
   ],
   "source": [
    "model_step = ModelStep(\n",
    "    \"Save Model\",\n",
    "    model = training_step.get_expected_model(),\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    result_path = \"$.ModelStepResults\"\n",
    ")\n",
    "\n",
    "# for deploying existing model\n",
    "existing_model_name = f\"dm-model-{uuid.uuid1().hex}\"\n",
    "existing_model = Model(\n",
    "    model_data = EXISTING_MODEL_URI,\n",
    "    image_uri = image_uri,\n",
    "    role = role,\n",
    "    name = existing_model_name\n",
    ")\n",
    "existing_model_step = ModelStep(\n",
    "    \"Existing Model\",\n",
    "    model = existing_model,\n",
    "    model_name = execution_input[\"ModelName\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Configuration Step\n",
    "\n",
    "> Endpoing Configuration Step won't be used in workflow as we demo Batch Transform in the lab.\n",
    "\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda function to check Endpoint Existed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/manual_pipeline/code/query_endpoint_existence.zip'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_endpoint_existence.zip'\n",
    "lambda_source_code = './code/query_endpoint_existence.py'\n",
    "\n",
    "\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "\n",
    "\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/{prefix}/code\",\n",
    "                  sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceConflictException",
     "evalue": "An error occurred (ResourceConflictException) when calling the CreateFunction operation: Function already exist: query_endpoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceConflictException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d9f7404e5e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mDescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Queries a SageMaker Endpoint existence.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mTimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mMemorySize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceConflictException\u001b[0m: An error occurred (ResourceConflictException) when calling the CreateFunction operation: Function already exist: query_endpoint"
     ]
    }
   ],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "function_name = 'query_endpoint'\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName = function_name,\n",
    "    Runtime = 'python3.7',\n",
    "    Role = role,\n",
    "    Handler = 'query_endpoint_existence.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket_name,\n",
    "        'S3Key': f'{prefix}/code/{zip_name}'\n",
    "    },\n",
    "    Description='Queries a SageMaker Endpoint existence.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_endpoint_lambda_step = LambdaStep(\n",
    "    'Query Endpoint Existence',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryEndpoint'],\n",
    "        'Payload':{\n",
    "            \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "deployed_endpoint_completed_lambda_step = LambdaStep(\n",
    "    'Query Deployed Endpoint Status',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryEndpoint'],\n",
    "        'Payload':{\n",
    "            \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Step\n",
    "\n",
    "> Endpoint Step won't be used in workflow as we demo Batch Transform in the lab.\n",
    "\n",
    "In the following cells, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is sucessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_creation_step = EndpointStep(\n",
    "    \"Create Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_update_step = EndpointStep(\n",
    "    \"Update Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_endpoint_status_step = Choice('Endpoint is InService?')\n",
    "\n",
    "endpoint_in_service_rule = ChoiceRule.StringEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_status'], value = 'InService')\n",
    "check_endpoint_status_step.add_choice(rule = endpoint_in_service_rule, next_step = endpoint_update_step)\n",
    "\n",
    "wait_step = Wait(state_id = f\"Wait Until Endpoint becomes InService\", seconds = 20)\n",
    "wait_step.next(query_endpoint_lambda_step)\n",
    "\n",
    "check_endpoint_status_step.default_choice(next_step = wait_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_endpoint_existence_step = Choice(\n",
    "    'Endpoint Existed?'\n",
    ")\n",
    "\n",
    "endpoint_existed_rule = ChoiceRule.BooleanEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_existed'], value = True)\n",
    "check_endpoint_existence_step.add_choice(rule = endpoint_existed_rule, next_step = check_endpoint_status_step)\n",
    "\n",
    "check_endpoint_existence_step.default_choice(next_step = endpoint_creation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query Deployed Endpoint Status LambdaStep(parameters={'FunctionName': <stepfunctions.inputs.placeholders.ExecutionInput object at 0x7faebf6d6eb8>, 'Payload': {'EndpointName.$': \"$$.Execution.Input['EndpointName']\"}}, resource='arn:aws:states:::lambda:invoke', type='Task')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check endpoint readiness\n",
    "deployed_endpoint_updating_step = Choice('Deployed Endpoint Status Updating?')\n",
    "\n",
    "wait_deployment_step = Wait(state_id = \"Wait Until Endpoint Deployment Completed\", seconds = 20)\n",
    "wait_deployment_step.next(deployed_endpoint_completed_lambda_step)\n",
    "\n",
    "deployed_endpoint_updating_rule = ChoiceRule.StringEquals(variable = deployed_endpoint_completed_lambda_step.output()['Payload']['endpoint_status'], value = 'Updating')\n",
    "deployed_endpoint_updating_step.add_choice(rule = deployed_endpoint_updating_rule, next_step = wait_deployment_step)\n",
    "\n",
    "final_step = Pass(state_id = 'Pass Step')\n",
    "\n",
    "deployed_endpoint_updating_step.default_choice(next_step = final_step)\n",
    "\n",
    "deployed_endpoint_completed_lambda_step.next(deployed_endpoint_updating_step)\n",
    "endpoint_creation_step.next(deployed_endpoint_completed_lambda_step)\n",
    "endpoint_update_step.next(deployed_endpoint_completed_lambda_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Workflow Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Fail` state to mark the workflow failed in case any of the steps fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_pipeline_step_failure = Fail(\n",
    "    \"ML Workflow Failed\", cause = \"SageMakerPipelineStepFailed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = Chain([training_step, model_step, endpoint_config_step, query_endpoint_lambda_step, check_endpoint_existence_step])\n",
    "deploy_existing_model_path = Chain([existing_model_step, endpoint_config_step, query_endpoint_lambda_step, check_endpoint_existence_step])\n",
    "# training_path = Chain([training_step, model_step, endpoint_config_step])\n",
    "# deploy_existing_model_path = Chain([existing_model_step, endpoint_config_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choice Step Configuration\n",
    "\n",
    "Now, we need to setup choice state for choose HPO / Training or not. See *Choice Rules* in the [AWS Step Functions Data Science SDK documentation](https://aws-step-functions-data-science-sdk.readthedocs.io) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_choice = Choice(\n",
    "    \"To do HPO?\"\n",
    ")\n",
    "training_choice = Choice(\n",
    "    \"To do Model Training?\"\n",
    ")\n",
    "\n",
    "# refer to execution input variable with required format - not user friendly.\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = True),\n",
    "    next_step = tuning_step                 \n",
    ")\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = False),\n",
    "    next_step = training_choice\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = True),\n",
    "    next_step = training_path\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = False),\n",
    "    next_step = deploy_existing_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Handling in the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_state_processing = Catch(\n",
    "    error_equals = [\"States.TaskFailed\"],\n",
    "    next_step = failed_state_sagemaker_pipeline_step_failure   \n",
    ")\n",
    "processing_step.add_catch(catch_state_processing)\n",
    "tuning_step.add_catch(catch_state_processing)\n",
    "training_step.add_catch(catch_state_processing)\n",
    "model_step.add_catch(catch_state_processing)\n",
    "endpoint_config_step.add_catch(catch_state_processing)\n",
    "endpoint_creation_step.add_catch(catch_state_processing)\n",
    "endpoint_update_step.add_catch(catch_state_processing)\n",
    "existing_model_step.add_catch(catch_state_processing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and execute the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution input parameter values\n",
    "preprocessing_job_name = f\"dm-preprocessing-{uuid.uuid1().hex}\"\n",
    "tuning_job_name = f\"dm-tuning\"\n",
    "training_job_name = f\"dm-training-{uuid.uuid1().hex}\"\n",
    "model_job_name = f\"dm-model-{uuid.uuid1().hex}\"\n",
    "# endpoint_job_name = f\"dm-endpoint-{uuid.uuid1().hex}\"\n",
    "endpoint_job_name = f\"dm-endpoint-manual\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "WORKFLOW_NAME = \"manaul-dm-ml-pipeline-6\"\n",
    "TO_DO_HPO = True\n",
    "TO_DO_TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfn_client = boto3.client('stepfunctions')\n",
    "\n",
    "workflow_role_arn = f\"arn:aws:states:{region}:{account_id}:stateMachine:{WORKFLOW_NAME}\"\n",
    "\n",
    "try:\n",
    "    response = sfn_client.describe_state_machine(\n",
    "        stateMachineArn = workflow_role_arn\n",
    "    )\n",
    "    existing_workflow = True\n",
    "except: \n",
    "    existing_workflow = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stepfunctions:Workflow updated successfully on AWS Step Functions. All execute() calls will use the updated definition and role within a few seconds. \n",
      "INFO:stepfunctions:Workflow execution started successfully on AWS Step Functions.\n"
     ]
    }
   ],
   "source": [
    "workflow_graph = Chain([processing_step, hpo_choice])\n",
    "# workflow_graph = Chain([hpo_choice])\n",
    "if existing_workflow:\n",
    "    # To update SFN workflow, need to do 'attach' & 'update' together.\n",
    "    workflow = Workflow.attach(state_machine_arn = workflow_role_arn)\n",
    "    workflow.update(definition = workflow_graph, role = WORKFLOW_EXECUTION_ROLE) \n",
    "    # Wait for 10s so that the update is completed before executing workflow\n",
    "    time.sleep(10)\n",
    "else:\n",
    "    workflow = Workflow(\n",
    "        name = WORKFLOW_NAME,\n",
    "        definition = workflow_graph,\n",
    "        role = WORKFLOW_EXECUTION_ROLE\n",
    "    )\n",
    "    workflow.create()\n",
    "    \n",
    "\n",
    "# execute workflow\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": TO_DO_HPO,\n",
    "        \"ToDoTraining\": TO_DO_TRAINING,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": function_name,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": lambda_function_query_hpo_job\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state_machine_advice(workflow_name, execution_id):\n",
    "    display(HTML(f'''<br>The Step Function workflow \"{workflow_name}\" is now executing... \n",
    "            <br>To view state machine in the console click \n",
    "            <a target=\"_blank\" href=\"https://{region}.console.aws.amazon.com/states/home?region={region}#/statemachines/view/arn:aws:states:ap-southeast-2:{account_id}:stateMachine:{workflow_name}\">State Machine</a> \n",
    "            <br>To view execution in the console click \n",
    "            <a target=\"_blank\" href=\"https://{region}.console.aws.amazon.com/states/home?region={region}#/executions/details/arn:aws:states:ap-southeast-2:{account_id}:execution:{workflow_name}:{execution_id}\">Execution</a>.\n",
    "        '''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execution.describe()\n",
    "execution_id = response['name']\n",
    "# advice state machine console link\n",
    "display_state_machine_advice(WORKFLOW_NAME, execution_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell multiple times to observe the workflow execution progress. Please note that the execution may take 15-20mins with using existing model for batch transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress(portrait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Experiment Analytics\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session = sagemaker_session, \n",
    "    experiment_name = experiment.experiment_name,\n",
    "    sort_order=\"Ascending\",\n",
    "    metric_names=['test:accuracy'],\n",
    "    parameter_names=['hidden_channels', 'epochs', 'dropout', 'optimizer']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>train - MediaType</th>\n",
       "      <th>train - Value</th>\n",
       "      <th>validation - MediaType</th>\n",
       "      <th>validation - Value</th>\n",
       "      <th>SageMaker.ModelArtifact - MediaType</th>\n",
       "      <th>SageMaker.ModelArtifact - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>...</th>\n",
       "      <th>code - MediaType</th>\n",
       "      <th>code - Value</th>\n",
       "      <th>input_data - MediaType</th>\n",
       "      <th>input_data - Value</th>\n",
       "      <th>test_data - MediaType</th>\n",
       "      <th>test_data - Value</th>\n",
       "      <th>train_data - MediaType</th>\n",
       "      <th>train_data - Value</th>\n",
       "      <th>validation_data - MediaType</th>\n",
       "      <th>validation_data - Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dm-training-e6320c922de711eba5af2dd7bc665570-a...</td>\n",
       "      <td>Training</td>\n",
       "      <td>arn:aws:sagemaker:ap-southeast-2:593380422482:...</td>\n",
       "      <td>csv</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "      <td>csv</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "      <td>[xgb-training-job-1606175914]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dm-preprocessing-e63207c42de711eba5af2dd7bc665...</td>\n",
       "      <td>Processing</td>\n",
       "      <td>arn:aws:sagemaker:ap-southeast-2:593380422482:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[xgb-processing-job-1606175913]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-ap-southeast-2-593380422482/sag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0  dm-training-e6320c922de711eba5af2dd7bc665570-a...    Training   \n",
       "1  dm-preprocessing-e63207c42de711eba5af2dd7bc665...  Processing   \n",
       "\n",
       "                                           SourceArn train - MediaType  \\\n",
       "0  arn:aws:sagemaker:ap-southeast-2:593380422482:...               csv   \n",
       "1  arn:aws:sagemaker:ap-southeast-2:593380422482:...               NaN   \n",
       "\n",
       "                                       train - Value validation - MediaType  \\\n",
       "0  s3://sagemaker-ap-southeast-2-593380422482/sag...                    csv   \n",
       "1                                                NaN                    NaN   \n",
       "\n",
       "                                  validation - Value  \\\n",
       "0  s3://sagemaker-ap-southeast-2-593380422482/sag...   \n",
       "1                                                NaN   \n",
       "\n",
       "   SageMaker.ModelArtifact - MediaType  \\\n",
       "0                                  NaN   \n",
       "1                                  NaN   \n",
       "\n",
       "                     SageMaker.ModelArtifact - Value  \\\n",
       "0  s3://sagemaker-ap-southeast-2-593380422482/sag...   \n",
       "1                                                NaN   \n",
       "\n",
       "                            Trials  ... code - MediaType  \\\n",
       "0    [xgb-training-job-1606175914]  ...              NaN   \n",
       "1  [xgb-processing-job-1606175913]  ...              NaN   \n",
       "\n",
       "                                        code - Value input_data - MediaType  \\\n",
       "0                                                NaN                    NaN   \n",
       "1  s3://sagemaker-ap-southeast-2-593380422482/sag...                    NaN   \n",
       "\n",
       "                                  input_data - Value test_data - MediaType  \\\n",
       "0                                                NaN                   NaN   \n",
       "1  s3://sagemaker-ap-southeast-2-593380422482/sag...                   NaN   \n",
       "\n",
       "                                   test_data - Value train_data - MediaType  \\\n",
       "0                                                NaN                    NaN   \n",
       "1  s3://sagemaker-ap-southeast-2-593380422482/sag...                    NaN   \n",
       "\n",
       "                                  train_data - Value  \\\n",
       "0                                                NaN   \n",
       "1  s3://sagemaker-ap-southeast-2-593380422482/sag...   \n",
       "\n",
       "  validation_data - MediaType  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "\n",
       "                             validation_data - Value  \n",
       "0                                                NaN  \n",
       "1  s3://sagemaker-ap-southeast-2-593380422482/sag...  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MetricName': 'validation:auc',\n",
       "  'Value': 0.7797179818153381,\n",
       "  'Timestamp': datetime.datetime(1970, 1, 19, 14, 3, 51, 986000, tzinfo=tzlocal())},\n",
       " {'MetricName': 'train:auc',\n",
       "  'Value': 0.7998440265655518,\n",
       "  'Timestamp': datetime.datetime(1970, 1, 19, 14, 3, 51, 986000, tzinfo=tzlocal())},\n",
       " {'MetricName': 'ObjectiveMetric',\n",
       "  'Value': 0.7797179818153381,\n",
       "  'Timestamp': datetime.datetime(1970, 1, 19, 14, 3, 51, 986000, tzinfo=tzlocal())}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = sm.describe_training_job(TrainingJobName = 'xgboost-201120-0017-007-fc507e21')\n",
    "response['FinalMetricDataList']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c1f00661-071d-4682-a0fe-55c4f03d85ab',\n",
       "  'HTTPStatusCode': 201,\n",
       "  'HTTPHeaders': {'date': 'Tue, 24 Nov 2020 01:31:10 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '940',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'c1f00661-071d-4682-a0fe-55c4f03d85ab'},\n",
       "  'RetryAttempts': 0},\n",
       " 'FunctionName': 'query_hpo_job',\n",
       " 'FunctionArn': 'arn:aws:lambda:ap-southeast-2:593380422482:function:query_hpo_job',\n",
       " 'Runtime': 'python3.7',\n",
       " 'Role': 'arn:aws:iam::593380422482:role/mlops-nyctaxi-sagemaker-role',\n",
       " 'Handler': 'query_hpo_job.lambda_handler',\n",
       " 'CodeSize': 1344,\n",
       " 'Description': 'Queries SageMaker HPO Job.',\n",
       " 'Timeout': 15,\n",
       " 'MemorySize': 128,\n",
       " 'LastModified': '2020-11-24T01:31:10.584+0000',\n",
       " 'CodeSha256': 'XUH1mlvb8DFPqetI0vpKZ76Bi5EcX4j8FLFu4hHtUbs=',\n",
       " 'Version': '$LATEST',\n",
       " 'TracingConfig': {'Mode': 'PassThrough'},\n",
       " 'RevisionId': '1b6a03b1-412d-4495-9865-7e9fd37779f2',\n",
       " 'State': 'Active',\n",
       " 'LastUpdateStatus': 'Successful'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['ModelArtifacts'])\n",
    "print(response['HyperParameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.describe_endpoint(EndpointName = 'dm-endpoint-manuala')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPO job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TrainingJobName': 'xgboost-201120-0017-007-fc507e21', 'TrainingJobArn': 'arn:aws:sagemaker:ap-southeast-2:593380422482:training-job/xgboost-201120-0017-007-fc507e21', 'CreationTime': datetime.datetime(2020, 11, 20, 0, 24, 2, tzinfo=tzlocal()), 'TrainingStartTime': datetime.datetime(2020, 11, 20, 0, 26, 2, tzinfo=tzlocal()), 'TrainingEndTime': datetime.datetime(2020, 11, 20, 0, 26, 35, tzinfo=tzlocal()), 'TrainingJobStatus': 'Completed', 'TunedHyperParameters': {'alpha': '1.9167548939755026', 'eta': '0.2513705646042541', 'max_depth': '4', 'min_child_weight': '2.561240034842159'}, 'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:auc', 'Value': 0.7797179818153381}, 'ObjectiveStatus': 'Succeeded'}\n",
      "{'MetricName': 'validation:auc', 'Value': 0.7797179818153381}\n"
     ]
    }
   ],
   "source": [
    "response = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName = 'xgboost-201120-0017'\n",
    ")\n",
    "print(response['BestTrainingJob'])\n",
    "print(response['BestTrainingJob']['FinalHyperParameterTuningJobObjectiveMetric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HyperParameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8cb2988c05f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HyperParameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'HyperParameters'"
     ]
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"MonitoringScheduleSummaries\": [\n",
      "        {\n",
      "            \"MonitoringScheduleName\": \"mlops-nyctaxi-pms-194d006e-c524-4ee3-bfd8-d60c4c3461cb\",\n",
      "            \"MonitoringScheduleArn\": \"arn:aws:sagemaker:ap-southeast-2:593380422482:monitoring-schedule/mlops-nyctaxi-pms-194d006e-c524-4ee3-bfd8-d60c4c3461cb\",\n",
      "            \"CreationTime\": 1605766012.023,\n",
      "            \"LastModifiedTime\": 1606176282.276,\n",
      "            \"MonitoringScheduleStatus\": \"Stopped\",\n",
      "            \"EndpointName\": \"mlops-nyctaxi-prd-194d006e-c524-4ee3-bfd8-d60c4c3461cb\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker list-monitoring-schedules --endpoint-name mlops-nyctaxi-prd-194d006e-c524-4ee3-bfd8-d60c4c3461cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker stop-monitoring-schedule --monitoring-schedule-name mlops-nyctaxi-pms-194d006e-c524-4ee3-bfd8-d60c4c3461cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!aws sagemaker delete-monitoring-schedule --monitoring-schedule-name mlops-nyctaxi-pms-194d006e-c524-4ee3-bfd8-d60c4c3461cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another workflow to create SNS notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TopicArn': 'arn:aws:sns:ap-southeast-2:593380422482:dm-model-training-notification-topic',\n",
       " 'ResponseMetadata': {'RequestId': 'ad5ec182-d25b-5095-b03f-99b48f23e633',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ad5ec182-d25b-5095-b03f-99b48f23e633',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '348',\n",
       "   'date': 'Tue, 24 Nov 2020 01:00:40 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SubscriptionArn': 'arn:aws:sns:ap-southeast-2:593380422482:dm-model-training-notification-topic:762aec58-0342-4654-9bd5-bb493dfc9555',\n",
       " 'ResponseMetadata': {'RequestId': '02f7f470-3799-5d28-8a97-ac4669d20c34',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '02f7f470-3799-5d28-8a97-ac4669d20c34',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '391',\n",
       "   'date': 'Tue, 24 Nov 2020 01:04:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/manual_pipeline/code/query_hpo_job.zip'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lambda function\n",
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_hpo_job.zip'\n",
    "lambda_source_code = './code/query_hpo_job.py'\n",
    "\n",
    "\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "\n",
    "\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/{prefix}/code\",\n",
    "                  sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "lambda_function_query_hpo_job = 'query_hpo_job'\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName = lambda_function_query_hpo_job,\n",
    "    Runtime = 'python3.7',\n",
    "    Role = role,\n",
    "    Handler = 'query_hpo_job.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket_name,\n",
    "        'S3Key': f'{prefix}/code/{zip_name}'\n",
    "    },\n",
    "    Description='Queries SageMaker HPO Job.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_hpo_job_lambda_step = LambdaStep(\n",
    "    'Query HPO Job',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryHpoJob'],\n",
    "        'Payload':{\n",
    "            \"HpoJobName.$\": \"$$.Execution.Input['TuningJobName']\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_job_sns_step = SnsPublishStep(\n",
    "    state_id = 'SNS Notification - HPO Job',\n",
    "    parameters = {\n",
    "        'TopicArn': topic_arn,\n",
    "        'Message': query_hpo_job_lambda_step.output()['Payload']['bestTrainingJob']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNS Notification - HPO Job SnsPublishStep(parameters={'TopicArn': 'arn:aws:sns:ap-southeast-2:593380422482:dm-model-training-notification-topic', 'Message': <stepfunctions.inputs.placeholders.StepInput object at 0x7faebe548978>}, resource='arn:aws:states:::sns:publish', type='Task')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_hpo_job_lambda_step.next(hpo_job_sns_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stepfunctions:Workflow created successfully on AWS Step Functions.\n",
      "INFO:stepfunctions:Workflow execution started successfully on AWS Step Functions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "workflow_graph = Chain([query_hpo_job_lambda_step])\n",
    "workflow = Workflow(\n",
    "    name = 'simple-hpo-job-notification',\n",
    "    definition = workflow_graph,\n",
    "    role = WORKFLOW_EXECUTION_ROLE\n",
    ")\n",
    "workflow.create()\n",
    "    \n",
    "\n",
    "# execute workflow\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": TO_DO_HPO,\n",
    "        \"ToDoTraining\": TO_DO_TRAINING,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": 'xgboost-201120-0017',\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": function_name,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": lambda_function_query_hpo_job\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
