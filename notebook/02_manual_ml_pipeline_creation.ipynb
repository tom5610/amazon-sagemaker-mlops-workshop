{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeting Direct Marketing Model Training ML Pipeline [manual]\n",
    "\n",
    "---\n",
    "\n",
    "Once you are familiar with using Amazon SageMaker built-in algorithm - [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to do [Targetting Direct Marketing model traing](./01_xgboost_direct_marketing_sagemaker.ipynb), we are going to build a ML Pipeline to automate the workflow with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). \n",
    "\n",
    "In the design:\n",
    "* There is preprocessing job to do data integration\n",
    "  * A table is created in Amazon Athena to query data on open air quality data. Visit [Open AQ](https://openaq.org/) for detail.\n",
    "  * A query to Amazon Athena to collect Sydney, Australia air quality data.\n",
    "  * Data cleansing and feature engineering\n",
    "  * Train and test data set are separated; we keep last 30 days' data as test set.\n",
    "  * Batch Transform test data is constructed based on the latest 100 record in test set. \n",
    "* Hyperparameters optimization is optional\n",
    "  * In pipeline, we will leave hyperparameter optimziation alone without doing batch transform.\n",
    "* Model training with tuned hyperparameters\n",
    "  * For example, you may collect the hyperparameters from HPO jobs with the best candidate.\n",
    "* Batch transform job is triggerred to forecast air quality.\n",
    "  * In the example, we do the batch inference for the latest 100 records in our test data set. \n",
    "\n",
    "In the notebook, we are going to demo how to create the workflow step by step and process till model training. Below is the related Step Functions workflow mapping to the ML pipeline with no HPO and using an trained model:\n",
    "\n",
    "![Direct Marketing](./img/air_quality_forecasting_ml_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Creation\n",
    "---\n",
    "To create ML pipeline, we will use Step Functions Data Science SDK v2.0.0rc1, which is compatible with SageMaker SDK 2.x.\n",
    "\n",
    "We will cover pipeline creation at below:\n",
    "* Environment initialization\n",
    "* Build Docker image for SageMaker Processing\n",
    "* Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" # 2.0.0\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0rc1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "\n",
    "import stepfunctions\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.steps.sagemaker import *\n",
    "from stepfunctions.steps.states import *\n",
    "from stepfunctions.steps.compute import *\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.steps import *\n",
    "from IPython.display import display, HTML, Javascript\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "\n",
    "session = boto3.Session()\n",
    "sm = session.client('sagemaker')\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm/manual_pipeline'\n",
    "account_id = session.client('sts').get_caller_identity().get('Account')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the workflow execution role. For the role arn, please refer to the output tab of the CloudFormation stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssm = boto3.client('ssm')\n",
    "# response = ssm.get_parameter(Name = \"/directmarketing/ml_pipeline/workflow_execution_role\")\n",
    "# WORKFLOW_EXECUTION_ROLE = response['Parameter']['Value']\n",
    "\n",
    "WORKFLOW_EXECUTION_ROLE = \"arn:aws:iam::593380422482:role/StepFunctionsWorkflowExecutionRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WORKFLOW_EXECUTION_ROLE:\n",
    "    raise Exception(\"ML Pipeline Parameters in System Manager is not setup properly. Please check whether the ml-pipeline stack has been created or not.\")\n",
    "else:\n",
    "    print(f\"Workflow execution IAM service role: {WORKFLOW_EXECUTION_ROLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTING_MODEL_URI = \"s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/output/xgboost-201120-0017-007-fc507e21/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./bank-additional.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)\n",
    "\n",
    "---\n",
    "\n",
    "#### Create Processing Step for data preprocessing\n",
    "\n",
    "We will now create the [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) that will launch a SageMaker Processing Job.\n",
    "\n",
    "In the processing job script `./pipeline/ml_pipeline_preprocessing.py`, the actions will be done:\n",
    "\n",
    "* Feature engineering on the dataset\n",
    "* Split training and test data \n",
    "* Store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_SCRIPT_LOCATION = \"./pipeline/preprocessing.py\"\n",
    "input_code_uri = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket = bucket_name,\n",
    "    key_prefix = f\"{prefix}/preprocessing/code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SKLearnProcessor` class lets you run a command inside the container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_processor = SKLearnProcessor(\n",
    "    framework_version='0.20.0',\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    max_runtime_in_seconds = 1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 locations of preprocessing output with training, test & all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = f\"s3://{bucket_name}/{prefix}/preprocessing/output\"\n",
    "processing_input_path = f's3://{bucket_name}/{prefix}/preprocessing/input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_file = './bank-additional/bank-additional-full.csv'\n",
    "sagemaker.s3.S3Uploader.upload(local_data_file, processing_input_path, sagemaker_session = sagemaker_session)\n",
    "input_data = f'{processing_input_path}/bank-additional-full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/manual_pipeline/preprocessing/input/bank-additional-full.csv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will use ScriptProcessor as defined in previous steps along with the inputs and outputs objects that are defined in the below steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name = \"code\",\n",
    "        source = input_code_uri,\n",
    "        destination = \"/opt/ml/processing/input/code\"\n",
    "    ),\n",
    "    ProcessingInput(\n",
    "        input_name = \"input_data\",\n",
    "        source = input_data,\n",
    "        destination='/opt/ml/processing/input'\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name = \"train_data\",\n",
    "        source = \"/opt/ml/processing/output/train\",\n",
    "        destination = f\"{output_data}/train\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name = \"validation_data\",\n",
    "        source = \"/opt/ml/processing/output/validation\",\n",
    "        destination = f\"{output_data}/validation\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name = \"test_data\",\n",
    "        source = \"/opt/ml/processing/output/test\",\n",
    "        destination = f\"{output_data}/test\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Execution parameters\n",
    "execution_input = ExecutionInput(\n",
    "    schema = {\n",
    "        \"PreprocessingJobName\": str,\n",
    "        \"ToDoHPO\": bool,\n",
    "        \"ToDoTraining\": bool,\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TuningJobName\": str,\n",
    "        \"ModelName\": str,\n",
    "        \"EndpointName\": str,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProcessingStep` queries open air quality data for Sydney Australia with Amazon Athena. Especially, we are using our bucket to store query result. In case you setup default workgroup in Amazon Athena, please ensure to uncheck ***Override client-side settings***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    \"DM Preprocessing Step\",\n",
    "    processor = preprocessing_processor,\n",
    "    job_name = execution_input[\"PreprocessingJobName\"],\n",
    "    inputs = inputs,\n",
    "    outputs = outputs,\n",
    "    container_arguments = [\"--data-file\", \"bank-additional-full.csv\"],\n",
    "    container_entrypoint = [\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Hyperparameter Tuning Step\n",
    "\n",
    "Setup tuning step and use choice state to decide whether we should do HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuning_output_path = f's3://{bucket_name}/{prefix}/tuning/output'\n",
    "image_uri = sagemaker.image_uris.retrieve(region = region, framework='xgboost', version='latest')\n",
    "\n",
    "tuning_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role, \n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    output_path = tuning_output_path,\n",
    "    sagemaker_session = sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set static hyperparameters\n",
    "The static parameters are the ones we know to be the best based on previously run HPO jobs, as well as the non-tunable parameters like prediction length and time frequency that are set according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = dict(\n",
    "    max_depth = 5,\n",
    "    eta = 0.2,\n",
    "    gamma = 4,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.8,\n",
    "    silent = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    num_round = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set hyper-parameter ranges\n",
    "The hyperparameter ranges define the parameters we want the runer to search across.\n",
    "\n",
    "> Explore: Look in the [user guide](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html) for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0, 1),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2),\n",
    "    'max_depth': IntegerParameter(1, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create HPO tunning job step\n",
    "Once we have the HPO tuner defined, we can define the tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_estimator.set_hyperparameters(**hpo)\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "hpo_tuner = HyperparameterTuner(\n",
    "    tuning_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3\n",
    ")\n",
    "\n",
    "hpo_data = dict(\n",
    "    train = f\"{output_data}/train\",\n",
    "    test = f\"{output_data}/validation\"\n",
    ")\n",
    "# as long as HPO is selected, wait for completion.\n",
    "tuning_step = TuningStep(\n",
    "    \"HPO Step\",\n",
    "    tuner = hpo_tuner,\n",
    "    job_name = execution_input[\"TuningJobName\"],\n",
    "    data = hpo_data,\n",
    "    wait_for_completion = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Training Step\n",
    "\n",
    "We create a DeepAR instance, which we will use to run a training job. This will be used to create a TrainingStep for the workflow.\n",
    "\n",
    "##### Setup the training job step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_path = f's3://{bucket_name}/{prefix}/training/output'\n",
    "training_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role, \n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    output_path = training_output_path,\n",
    "    sagemaker_session = sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'_tuning_objective_metric': 'validation:auc',\n",
    " 'alpha': '1.9167548939755026',\n",
    " 'eta': '0.2513705646042541',\n",
    " 'gamma': '4',\n",
    " 'max_depth': '4',\n",
    " 'min_child_weight': '2.561240034842159',\n",
    " 'num_round': '100',\n",
    " 'objective': 'binary:logistic',\n",
    " 'silent': '0',\n",
    " 'subsample': '0.8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyper parameters for tuning\n",
    "hpo = dict(\n",
    "    max_depth = 5,\n",
    "    eta = 0.2,\n",
    "    gamma = 4,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.8,\n",
    "    silent = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    num_round = 100\n",
    ")\n",
    "training_estimator.set_hyperparameters(**hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/train', content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/validation', content_type='csv')\n",
    "\n",
    "training_data = dict(\n",
    "    train = s3_input_train,\n",
    "    validation = s3_input_validation\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    \"Training Step\",\n",
    "    estimator = training_estimator,\n",
    "    data = training_data,\n",
    "    job_name = execution_input[\"TrainingJobName\"],\n",
    "    wait_for_completion = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Step\n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = ModelStep(\n",
    "    \"Save Model\",\n",
    "    model = training_step.get_expected_model(),\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    result_path = \"$.ModelStepResults\"\n",
    ")\n",
    "\n",
    "# for deploying existing model\n",
    "existing_model_name = f\"dm-model-{uuid.uuid1().hex}\"\n",
    "existing_model = Model(\n",
    "    model_data = EXISTING_MODEL_URI,\n",
    "    image_uri = image_uri,\n",
    "    role = role,\n",
    "    name = existing_model_name\n",
    ")\n",
    "existing_model_step = ModelStep(\n",
    "    \"Existing Model\",\n",
    "    model = existing_model,\n",
    "    model_name = execution_input[\"ModelName\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Configuration Step\n",
    "\n",
    "> Endpoing Configuration Step won't be used in workflow as we demo Batch Transform in the lab.\n",
    "\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda function to check Endpoint Existed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_endpoint_existence.zip'\n",
    "lambda_source_code = './code/query_endpoint_existence.py'\n",
    "\n",
    "\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "\n",
    "\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/{prefix}/code\",\n",
    "                  sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "function_name = 'query_endpoint'\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName = function_name,\n",
    "    Runtime = 'python3.7',\n",
    "    Role = role,\n",
    "    Handler = 'query_endpoint_existence.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket_name,\n",
    "        'S3Key': f'{prefix}/code/{zip_name}'\n",
    "    },\n",
    "    Description='Queries a SageMaker Endpoint existence.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_endpoint_lambda_step = LambdaStep(\n",
    "    'Query Endpoint Existence',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryEndpoint'],\n",
    "        'Payload':{\n",
    "            \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "query_endpoint_status_lambda_step = LambdaStep(\n",
    "    'Query Endpoint Status',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryEndpoint'],\n",
    "        'Payload':{\n",
    "            \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Step\n",
    "\n",
    "> Endpoint Step won't be used in workflow as we demo Batch Transform in the lab.\n",
    "\n",
    "In the following cells, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is sucessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_ceation_step = EndpointStep(\n",
    "    \"Create Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_update_step = EndpointStep(\n",
    "    \"Update Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_endpoint_existence_step = Choice(\n",
    "    'Endpoint Existed?'\n",
    ")\n",
    "\n",
    "endpoint_existed_rule = ChoiceRule.BooleanEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_existed'], value = True)\n",
    "check_endpoint_existence_step.add_choice(rule = endpoint_existed_rule, next_step = query_endpoint_status_lambda_step)\n",
    "\n",
    "check_endpoint_existence_step.default_choice(next_step = endpoint_ceation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_endpoint_status_step = Choice('Endpoint is InService?')\n",
    "\n",
    "wait_step = Wait(state_id = f\"Wait Until Endpoint becomes InService\", seconds = 20)\n",
    "wait_step.next(query_endpoint_status_lambda_step)\n",
    "\n",
    "endpoint_in_service_rule = ChoiceRule.StringEquals(variable = query_endpoint_status_lambda_step.output()['Payload']['endpoint_status'], value = 'InService')\n",
    "check_endpoint_status_step.add_choice(rule = endpoint_in_service_rule, next_step = endpoint_update_step)\n",
    "\n",
    "check_endpoint_status_step.default_choice(next_step = wait_step)\n",
    "\n",
    "query_endpoint_status_lambda_step.next(check_endpoint_status_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Workflow Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Fail` state to mark the workflow failed in case any of the steps fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_pipeline_step_failure = Fail(\n",
    "    \"ML Workflow Failed\", cause = \"SageMakerPipelineStepFailed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = Chain([training_step, model_step, endpoint_config_step, query_endpoint_lambda_step, check_endpoint_existence_step])\n",
    "deploy_existing_model_path = Chain([existing_model_step, endpoint_config_step, query_endpoint_lambda_step, check_endpoint_existence_step])\n",
    "# training_path = Chain([training_step, model_step, endpoint_config_step])\n",
    "# deploy_existing_model_path = Chain([existing_model_step, endpoint_config_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choice Step Configuration\n",
    "\n",
    "Now, we need to setup choice state for choose HPO / Training or not. See *Choice Rules* in the [AWS Step Functions Data Science SDK documentation](https://aws-step-functions-data-science-sdk.readthedocs.io) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_choice = Choice(\n",
    "    \"To do HPO?\"\n",
    ")\n",
    "training_choice = Choice(\n",
    "    \"To do Model Training?\"\n",
    ")\n",
    "\n",
    "# refer to execution input variable with required format - not user friendly.\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = True),\n",
    "    next_step = tuning_step                 \n",
    ")\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = False),\n",
    "    next_step = training_choice\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = True),\n",
    "    next_step = training_path\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = False),\n",
    "    next_step = deploy_existing_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Handling in the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_state_processing = Catch(\n",
    "    error_equals = [\"States.TaskFailed\"],\n",
    "    next_step = failed_state_sagemaker_pipeline_step_failure   \n",
    ")\n",
    "processing_step.add_catch(catch_state_processing)\n",
    "tuning_step.add_catch(catch_state_processing)\n",
    "training_step.add_catch(catch_state_processing)\n",
    "model_step.add_catch(catch_state_processing)\n",
    "endpoint_config_step.add_catch(catch_state_processing)\n",
    "endpoint_ceation_step.add_catch(catch_state_processing)\n",
    "endpoint_update_step.add_catch(catch_state_processing)\n",
    "existing_model_step.add_catch(catch_state_processing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and execute the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution input parameter values\n",
    "preprocessing_job_name = f\"dm-preprocessing-{uuid.uuid1().hex}\"\n",
    "tuning_job_name = f\"dm-tuning-{uuid.uuid1().hex}\"\n",
    "training_job_name = f\"dm-training-{uuid.uuid1().hex}\"\n",
    "model_job_name = f\"dm-model-{uuid.uuid1().hex}\"\n",
    "# endpoint_job_name = f\"dm-endpoint-{uuid.uuid1().hex}\"\n",
    "endpoint_job_name = f\"dm-endpoint-manual\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "WORKFLOW_NAME = \"manaul-dm-ml-pipeline-6\"\n",
    "TO_DO_HPO = False\n",
    "TO_DO_TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfn_client = boto3.client('stepfunctions')\n",
    "\n",
    "workflow_role_arn = f\"arn:aws:states:{region}:{account_id}:stateMachine:{WORKFLOW_NAME}\"\n",
    "\n",
    "try:\n",
    "    response = sfn_client.describe_state_machine(\n",
    "        stateMachineArn = workflow_role_arn\n",
    "    )\n",
    "    existing_workflow = True\n",
    "except: \n",
    "    existing_workflow = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workflow_graph = Chain([processing_step, hpo_choice])\n",
    "workflow_graph = Chain([hpo_choice])\n",
    "if existing_workflow:\n",
    "    # To update SFN workflow, need to do 'attach' & 'update' together.\n",
    "    workflow = Workflow.attach(state_machine_arn = workflow_role_arn)\n",
    "    workflow.update(definition = workflow_graph, role = WORKFLOW_EXECUTION_ROLE) \n",
    "    # Wait for 10s so that the update is completed before executing workflow\n",
    "    time.sleep(10)\n",
    "else:\n",
    "    workflow = Workflow(\n",
    "        name = WORKFLOW_NAME,\n",
    "        definition = workflow_graph,\n",
    "        role = WORKFLOW_EXECUTION_ROLE\n",
    "    )\n",
    "    workflow.create()\n",
    "    \n",
    "\n",
    "# execute workflow\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": TO_DO_HPO,\n",
    "        \"ToDoTraining\": TO_DO_TRAINING,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": function_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state_machine_advice(workflow_name, execution_id):\n",
    "    display(HTML(f'''<br>The Step Function workflow \"{workflow_name}\" is now executing... \n",
    "            <br>To view state machine in the console click \n",
    "            <a target=\"_blank\" href=\"https://{region}.console.aws.amazon.com/states/home?region={region}#/statemachines/view/arn:aws:states:ap-southeast-2:{account_id}:stateMachine:{workflow_name}\">State Machine</a> \n",
    "            <br>To view execution in the console click \n",
    "            <a target=\"_blank\" href=\"https://{region}.console.aws.amazon.com/states/home?region={region}#/executions/details/arn:aws:states:ap-southeast-2:{account_id}:execution:{workflow_name}:{execution_id}\">Execution</a>.\n",
    "        '''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execution.describe()\n",
    "execution_id = response['name']\n",
    "# advice state machine console link\n",
    "display_state_machine_advice(WORKFLOW_NAME, execution_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell multiple times to observe the workflow execution progress. Please note that the execution may take 15-20mins with using existing model for batch transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress(portrait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.describe_training_job(TrainingJobName = 'xgboost-201120-0017-007-fc507e21')\n",
    "response['FinalMetricDataList']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['ModelArtifacts'])\n",
    "print(response['HyperParameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.describe_endpoint(EndpointName = 'dm-endpoint-manuala')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPO job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName = 'xgboost-201120-0017'\n",
    ")\n",
    "print(response['BestTrainingJob'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['HyperParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
