version: 0.2

env:
  variables:
    SUB_PATH: notebook
  parameter-store:
    WORKFLOW_EXECUTION_ROLE: /ml_pipeline/workflow_execution_role
    WORKFLOW_NAME: /ml_pipeline/workflow_name
    REQUIRE_HPO: /ml_pipeline/require_hpo
    REQUIRE_MODEL_TRAINING: /ml_pipeline/require_model_training
    NOTIFICATION_TOPIC_NAME: /ml_pipeline/workflow_notification_topic_name
    MODEL_TRAINING_S3_BUCKET_NAME: /ml_pipeline/model_training_s3_bucket_name
    QUERY_ENDPOINT_LAMBDA_FUNCTION_NAME: /ml_pipeline/query_endpoint
    QUERY_HPO_JOB_LAMBDA_FUNCTION_NAME: /ml_pipeline/query_hpo_job

phases:
  install:
    runtime-versions:
      python: 3.7
    commands:
      - echo "Installing requirements"
      - pip install -r $CODEBUILD_SRC_DIR/$SUB_PATH/pipeline/requirements.txt

  pre_build:
    commands:
      - echo List source files
      - echo Get pipeline name
      - export PIPELINE_NAME=${CODEBUILD_INITIATOR#codepipeline}
      - echo $PIPELINE_NAME
      - echo $ECR_REPO_NAME is the target preprocessing container repo name.
      - echo Build preprocessing docker container
      - cd $CODEBUILD_SRC_DIR/$SUB_PATH
      - echo Setup default model artifact.
      - python3 ./pipeline/ml_pipeline_dependencies.py --bucket-name ${MODEL_TRAINING_S3_BUCKET_NAME}
      - echo Display the data source files
      - cd $CODEBUILD_SRC_DIR_DataSourceOutput
      - ls -lstra
      - aws s3 cp ./bank-additional-full.csv s3://${MODEL_TRAINING_S3_BUCKET_NAME}/preprocessing/input/bank-additional-full.csv

  build:
    commands:
      - echo Build started on `date`
      - cd $CODEBUILD_SRC_DIR/$SUB_PATH
      - echo Create or Update Workflow - State Machine in Step Functions
      - python3 ./pipeline/ml_pipeline.py --workflow-name ${WORKFLOW_NAME} --workflow-execution-role ${WORKFLOW_EXECUTION_ROLE} --data-file  "bank-additional-full.csv" --topic-name ${NOTIFICATION_TOPIC_NAME} --bucket-name ${MODEL_TRAINING_S3_BUCKET_NAME} 
      - echo Create Workflow Execution Input file
      - python3 ./pipeline/ml_pipeline_input.py --require-hpo ${REQUIRE_HPO} --require-model-training ${REQUIRE_MODEL_TRAINING}  --query-endpoint-lambda-function-name ${QUERY_ENDPOINT_LAMBDA_FUNCTION_NAME} --query-hpo-job-lambda-function-name ${QUERY_HPO_JOB_LAMBDA_FUNCTION_NAME} --input-file-path "./input.json"
      - echo $CODEBUILD_RESOLVED_SOURCE_VERSION > commit.txt
      - echo $CODEBUILD_BUILD_ID >> commit.txt # Add build ID when commit doesn't change

  post_build:
    commands:
      - echo Package custom resources
      - echo Done

artifacts:
  files:
    - $SUB_PATH/*.json
    - $SUB_PATH/*.txt
  discard-paths: yes