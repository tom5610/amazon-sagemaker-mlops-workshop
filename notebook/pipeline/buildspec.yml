version: 0.2

env:
  variables:
    SUB_PATH: notebook
  parameter-store:
    WORKFLOW_EXECUTION_ROLE: /ml_pipeline/workflow_execution_role
    WORKFLOW_NAME: /ml_pipeline/workflow_name
    REQUIRE_HPO: /ml_pipeline/require_hpo
    REQUIRE_MODEL_TRAINING: /ml_pipeline/require_model_training
    NOTIFICATION_TOPIC_NAME: /ml_pipeline/workflow_notification_topic_name
    MODEL_TRAINING_S3_BUCKET_NAME: /ml_pipeline/model_training_s3_bucket_name

phases:
  install:
    runtime-versions:
      python: 3.7
    commands:
      - echo "Installing requirements"
      - pip install -r $CODEBUILD_SRC_DIR/$SUB_PATH/pipeline/requirements.txt

  pre_build:
    commands:
      - echo List source files
      - echo Get pipeline name
      - export PIPELINE_NAME=${CODEBUILD_INITIATOR#codepipeline}
      - echo $PIPELINE_NAME
      - echo $ECR_REPO_NAME is the target preprocessing container repo name.
      - echo Build preprocessing docker container
      - cd $CODEBUILD_SRC_DIR/$SUB_PATH
      - echo Setup default model artifact.
      - python3 ./pipeline/ml_pipeline_dependencies.py --bucket-name ${MODEL_TRAINING_S3_BUCKET_NAME}
      - echo Display the data source files
      - pwd
      - ls -lstra

  build:
    commands:
      - echo Build started on `date`
      - cd $CODEBUILD_SRC_DIR/$SUB_PATH
      - echo Create ML Pipeline
      - python3 ./pipeline/ml_pipeline.py --workflow-name ${WORKFLOW_NAME} --workflow-execution-role ${WORKFLOW_EXECUTION_ROLE} --data-file  "bank-additional-full.csv" --topic-name ${NOTIFICATION_TOPIC_NAME} --require-hpo ${REQUIRE_HPO} --require-model-training ${REQUIRE_MODEL_TRAINING}  --bucket-name ${MODEL_TRAINING_S3_BUCKET_NAME}
      - echo $CODEBUILD_RESOLVED_SOURCE_VERSION > commit.txt
      - echo $CODEBUILD_BUILD_ID >> commit.txt # Add build ID when commit doesn't change

  post_build:
    commands:
      - echo Package custom resources
      - echo Done

artifacts:
  files:
    - $SUB_PATH/*.json
    - $SUB_PATH/*.yml
  discard-paths: yes