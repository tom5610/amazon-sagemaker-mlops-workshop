{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeting Direct Marketing Model Training ML Pipeline [manual]\n",
    "\n",
    "---\n",
    "\n",
    "Once you are familiar with using Amazon SageMaker built-in algorithm - [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to do [Targetting Direct Marketing model traing](./01_xgboost_direct_marketing_sagemaker.ipynb), we are going to build a ML Pipeline to automate the workflow with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). \n",
    "\n",
    "In the design:\n",
    "* Preprocessing Job for feature engineering\n",
    "* Model training with tuned hyperparameters\n",
    "  * For example, you may collect the hyperparameters from HPO jobs with the best candidate.\n",
    "* Hyperparameters optimization is optional\n",
    "\n",
    "In the notebook, we are going to demo how to create the workflow step by step. Below is the related Step Functions workflow mapping to the ML pipeline with no HPO and using an trained model:\n",
    "\n",
    "![Direct Marketing](./images/dm_ml_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Creation\n",
    "---\n",
    "To create ML pipeline, we will use Step Functions Data Science SDK v2.0.0rc1, which is compatible with SageMaker SDK 2.x.\n",
    "\n",
    "We will cover pipeline creation at below:\n",
    "* Environment initialization\n",
    "* Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" # 2.0.0\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0rc1\"\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the workflow execution role. For the role arn, please refer to the output tab of the CloudFormation stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssm = boto3.client('ssm')\n",
    "# response = ssm.get_parameter(Name = \"/directmarketing/ml_pipeline/workflow_execution_role\")\n",
    "# WORKFLOW_EXECUTION_ROLE = response['Parameter']['Value']\n",
    "\n",
    "WORKFLOW_EXECUTION_ROLE = \"arn:aws:iam::593380422482:role/StepFunctionsWorkflowExecutionRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow execution IAM service role: arn:aws:iam::593380422482:role/StepFunctionsWorkflowExecutionRole\n"
     ]
    }
   ],
   "source": [
    "if not WORKFLOW_EXECUTION_ROLE:\n",
    "    raise Exception(\"ML Pipeline Parameters in System Manager is not setup properly. Please check whether the ml-pipeline stack has been created or not.\")\n",
    "else:\n",
    "    print(f\"Workflow execution IAM service role: {WORKFLOW_EXECUTION_ROLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTING_MODEL_URI = \"s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-dm/output/xgboost-201120-0017-007-fc507e21/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.ml_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_input_path = f's3://{bucket_name}/preprocessing/input'\n",
    "local_data_file = './bank-additional/bank-additional-full.csv'\n",
    "sagemaker.s3.S3Uploader.upload(local_data_file, processing_input_path, sagemaker_session = sagemaker_session)\n",
    "input_data = f'{processing_input_path}/bank-additional-full.csv'\n",
    "\n",
    "topic_name = 'dm-model-training-notification-topic'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "experiment = create_experiment(f\"xgboost-target-direct-marketing-{suffix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.ml_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:stepfunctions:Workflow updated successfully on AWS Step Functions. All execute() calls will use the updated definition and role within a few seconds. \n",
      "INFO:stepfunctions:Workflow execution started successfully on AWS Step Functions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "require_hpo = True\n",
    "require_model_training = True \n",
    "data_file =    \"bank-additional-full.csv\" \n",
    "workflow_name =    \"manual-ml-pipeline-dm\" \n",
    "workflow_execution_role =    WORKFLOW_EXECUTION_ROLE\n",
    "\n",
    "# bucket_name is created in ml_pipeline_dependencies.py, which is imported at the beginning.\n",
    "workflow = create_workflow(\n",
    "    bucket_name, \n",
    "    data_file,\n",
    "    topic_name,\n",
    "    experiment.experiment_name,\n",
    "    EXISTING_MODEL_URI,\n",
    "    workflow_name, \n",
    "    region, \n",
    "    account_id,\n",
    "    workflow_execution_role,\n",
    "    sagemaker_execution_role\n",
    ")\n",
    "\n",
    "# execute workflow\n",
    "suffix = datetime.now().strftime(\"%y%m%d-%H%M\")\n",
    "\n",
    "# execution input parameter values\n",
    "preprocessing_job_name = f\"dm-preprocessing-{suffix}\"\n",
    "tuning_job_name = f\"dm-tuning-{suffix}\"\n",
    "training_job_name = f\"dm-training-{suffix}\"\n",
    "model_job_name = f\"dm-model-{suffix}\"\n",
    "endpoint_config_name = f\"dm-endpoint-config-{suffix}\"\n",
    "endpoint_job_name = \"direct-marketing-endpoint\"\n",
    "lambda_function_query_endpoint = 'query_endpoint'\n",
    "lambda_function_query_hpo_job = 'query_hpo_job'\n",
    "\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": require_hpo,\n",
    "        \"ToDoTraining\": require_model_training,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointConfigName\": endpoint_config_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": lambda_function_query_endpoint,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": lambda_function_query_hpo_job\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./bank-additional.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML Pipline with Step Functions Data Science SDK (v2.0.0rc1)\n",
    "\n",
    "---\n",
    "\n",
    "#### Create Processing Step for data preprocessing\n",
    "\n",
    "We will now create the [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) that will launch a SageMaker Processing Job.\n",
    "\n",
    "In the processing job script `./pipeline/preprocessing.py`, the actions will be done:\n",
    "\n",
    "* Feature engineering on the dataset\n",
    "* Split training and test data \n",
    "* Store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_SCRIPT_LOCATION = \"./pipeline/preprocessing.py\"\n",
    "input_code_uri = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket = bucket_name,\n",
    "    key_prefix = f\"{prefix}/preprocessing/code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SKLearnProcessor` class lets you run a command inside the container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_processor = SKLearnProcessor(\n",
    "    framework_version='0.20.0',\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    max_runtime_in_seconds = 1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 locations of preprocessing output with training, test & all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = f\"s3://{bucket_name}/{prefix}/preprocessing/output\"\n",
    "processing_input_path = f's3://{bucket_name}/{prefix}/preprocessing/input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will use `SKLearnProcess` as defined in previous steps along with the inputs and outputs objects that are defined in the below steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name = \"code\",\n",
    "        source = input_code_uri,\n",
    "        destination = \"/opt/ml/processing/input/code\"\n",
    "    ),\n",
    "    ProcessingInput(\n",
    "        input_name = \"input_data\",\n",
    "        source = input_data,\n",
    "        destination='/opt/ml/processing/input'\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name = \"train_data\",\n",
    "        source = \"/opt/ml/processing/output/train\",\n",
    "        destination = f\"{output_data}/train\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name = \"validation_data\",\n",
    "        source = \"/opt/ml/processing/output/validation\",\n",
    "        destination = f\"{output_data}/validation\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name = \"test_data\",\n",
    "        source = \"/opt/ml/processing/output/test\",\n",
    "        destination = f\"{output_data}/test\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Execution parameters\n",
    "execution_input = ExecutionInput(\n",
    "    schema = {\n",
    "        \"PreprocessingJobName\": str,\n",
    "        \"ToDoHPO\": bool,\n",
    "        \"ToDoTraining\": bool,\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TuningJobName\": str,\n",
    "        \"ModelName\": str,\n",
    "        \"EndpointName\": str,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": str,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Experiment\n",
    "experiment = Experiment.create(\n",
    "    experiment_name = f\"xgboost-target-direct-marketing-{suffix}\", \n",
    "    description = \"Classification of target direct marketing\", \n",
    "    sagemaker_boto_client = sm)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = f\"xgb-processing-job-{suffix}\"\n",
    "xgb_trial = Trial.create(\n",
    "    trial_name = trial_name, \n",
    "    experiment_name = experiment.experiment_name,\n",
    "    sagemaker_boto_client = sm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProcessingStep` queries open air quality data for Sydney Australia with Amazon Athena. Especially, we are using our bucket to store query result. In case you setup default workgroup in Amazon Athena, please ensure to uncheck ***Override client-side settings***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    \"Preprocessing\",\n",
    "    processor = preprocessing_processor,\n",
    "    job_name = execution_input[\"PreprocessingJobName\"],\n",
    "    inputs = inputs,\n",
    "    outputs = outputs,\n",
    "    container_arguments = [\"--data-file\", \"bank-additional-full.csv\"],\n",
    "    container_entrypoint = [\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"],\n",
    "    experiment_config = {\n",
    "        \"TrialName\": xgb_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Processing\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Hyperparameter Tuning Step\n",
    "\n",
    "Setup tuning step and use choice state to decide whether we should do HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuning_output_path = f's3://{bucket_name}/{prefix}/tuning/output'\n",
    "image_uri = sagemaker.image_uris.retrieve(region = region, framework='xgboost', version='latest')\n",
    "\n",
    "tuning_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role, \n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    output_path = tuning_output_path,\n",
    "    sagemaker_session = sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set static hyperparameters\n",
    "The static parameters are the ones we know to be the best based on previously run HPO jobs, as well as the non-tunable parameters like prediction length and time frequency that are set according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = dict(\n",
    "    max_depth = 5,\n",
    "    eta = 0.2,\n",
    "    gamma = 4,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.8,\n",
    "    silent = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    num_round = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set hyper-parameter ranges\n",
    "The hyperparameter ranges define the parameters we want the runer to search across.\n",
    "\n",
    "> Explore: Look in the [user guide](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html) for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0, 1),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2),\n",
    "    'max_depth': IntegerParameter(1, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create HPO tunning job step\n",
    "Once we have the HPO tuner defined, we can define the tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_estimator.set_hyperparameters(**hpo)\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "hpo_tuner = HyperparameterTuner(\n",
    "    tuning_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/train', content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/validation', content_type='csv')\n",
    "\n",
    "hpo_data = dict(\n",
    "    train = s3_input_train,\n",
    "    validation = s3_input_validation\n",
    ")\n",
    "\n",
    "# as long as HPO is selected, wait for completion.\n",
    "tuning_step = TuningStep(\n",
    "    \"HPO Step\",\n",
    "    tuner = hpo_tuner,\n",
    "    job_name = execution_input[\"TuningJobName\"],\n",
    "    data = hpo_data,\n",
    "    wait_for_completion = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function\n",
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_hpo_job.zip'\n",
    "lambda_source_code = './code/query_hpo_job.py'\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/{prefix}/code\",\n",
    "                  sagemaker_session = sagemaker_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "lambda_function_query_hpo_job = 'query_hpo_job'\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName = lambda_function_query_hpo_job,\n",
    "    Runtime = 'python3.7',\n",
    "    Role = role,\n",
    "    Handler = 'query_hpo_job.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket_name,\n",
    "        'S3Key': f'{prefix}/code/{zip_name}'\n",
    "    },\n",
    "    Description='Queries SageMaker HPO Job.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_hpo_job_lambda_step = LambdaStep(\n",
    "    'Query HPO Job',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryHpoJob'],\n",
    "        'Payload':{\n",
    "            \"HpoJobName.$\": \"$$.Execution.Input['TuningJobName']\"\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SNS Topic and Complete Subscription\n",
    "sns = boto3.client('sns')\n",
    "topic_name = 'dm-model-training-notification-topic'\n",
    "response = sns.create_topic(Name = topic_name)\n",
    "\n",
    "topic_arn = response['TopicArn']\n",
    "email_id = 'tomlu@amazon.com'\n",
    "\n",
    "response = sns.subscribe(\n",
    "    TopicArn = topic_arn,\n",
    "    Protocol = 'email',\n",
    "    Endpoint = email_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_job_sns_step = SnsPublishStep(\n",
    "    state_id = 'SNS Notification - HPO Job',\n",
    "    parameters = {\n",
    "        'TopicArn': topic_arn,\n",
    "        'Message': query_hpo_job_lambda_step.output()['Payload']['bestTrainingJob']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Training Step\n",
    "\n",
    "We create a DeepAR instance, which we will use to run a training job. This will be used to create a TrainingStep for the workflow.\n",
    "\n",
    "##### Setup the training job step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_path = f's3://{bucket_name}/{prefix}/training/output'\n",
    "training_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role, \n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    output_path = training_output_path,\n",
    "    sagemaker_session = sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'_tuning_objective_metric': 'validation:auc',\n",
    " 'alpha': '1.9167548939755026',\n",
    " 'eta': '0.2513705646042541',\n",
    " 'gamma': '4',\n",
    " 'max_depth': '4',\n",
    " 'min_child_weight': '2.561240034842159',\n",
    " 'num_round': '100',\n",
    " 'objective': 'binary:logistic',\n",
    " 'silent': '0',\n",
    " 'subsample': '0.8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = dict(\n",
    "    max_depth = 5,\n",
    "    eta = 0.2,\n",
    "    gamma = 4,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.8,\n",
    "    silent = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    num_round = 100\n",
    ")\n",
    "training_estimator.set_hyperparameters(**hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/train', content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=f'{output_data}/validation', content_type='csv')\n",
    "\n",
    "training_data = dict(\n",
    "    train = s3_input_train,\n",
    "    validation = s3_input_validation\n",
    ")\n",
    "\n",
    "trial_name = f\"xgb-training-job-{int(time.time())}\"\n",
    "xgb_trial = Trial.create(\n",
    "    trial_name = trial_name, \n",
    "    experiment_name = experiment.experiment_name,\n",
    "    sagemaker_boto_client = sm,\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    \"Model Training\",\n",
    "    estimator = training_estimator,\n",
    "    data = training_data,\n",
    "    job_name = execution_input[\"TrainingJobName\"],\n",
    "    wait_for_completion = True,\n",
    "    experiment_config = {\n",
    "        \"TrialName\": xgb_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Step\n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = ModelStep(\n",
    "    \"Save Model\",\n",
    "    model = training_step.get_expected_model(),\n",
    "    model_name = execution_input[\"ModelName\"]\n",
    ")\n",
    "\n",
    "# for deploying existing model\n",
    "existing_model_name = f\"dm-model-{uuid.uuid1().hex}\"\n",
    "existing_model = Model(\n",
    "    model_data = EXISTING_MODEL_URI,\n",
    "    image_uri = image_uri,\n",
    "    role = role,\n",
    "    name = existing_model_name\n",
    ")\n",
    "existing_model_step = ModelStep(\n",
    "    \"Existing Model\",\n",
    "    model = existing_model,\n",
    "    model_name = execution_input[\"ModelName\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Configuration Step\n",
    "\n",
    "> Endpoing Configuration Step won't be used in workflow as we demo Batch Transform in the lab.\n",
    "\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda function to check Endpoint Existed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from sagemaker.s3 import S3Uploader\n",
    "zip_name = 'query_endpoint_existence.zip'\n",
    "lambda_source_code = './code/query_endpoint_existence.py'\n",
    "\n",
    "\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "\n",
    "\n",
    "S3Uploader.upload(local_path = zip_name, \n",
    "                  desired_s3_uri = f\"s3://{bucket_name}/{prefix}/code\",\n",
    "                  sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "lambda_function_query_endpoint = 'query_endpoint'\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName = lambda_function_query_endpoint,\n",
    "    Runtime = 'python3.7',\n",
    "    Role = role,\n",
    "    Handler = 'query_endpoint_existence.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket_name,\n",
    "        'S3Key': f'{prefix}/code/{zip_name}'\n",
    "    },\n",
    "    Description='Queries a SageMaker Endpoint existence.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_endpoint_lambda_step = LambdaStep(\n",
    "    'Query Endpoint Existence',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryEndpoint'],\n",
    "        'Payload':{\n",
    "            \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "deployed_endpoint_completed_lambda_step = LambdaStep(\n",
    "    'Query Endpoint Deployment Status',\n",
    "    parameters = {  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionNameOfQueryEndpoint'],\n",
    "        'Payload':{\n",
    "            \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Step\n",
    "\n",
    "> Endpoint Step won't be used in workflow as we demo Batch Transform in the lab.\n",
    "\n",
    "In the following cells, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is sucessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_creation_step = EndpointStep(\n",
    "    \"Create Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_update_step = EndpointStep(\n",
    "    \"Update Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_endpoint_status_step = Choice('Endpoint is InService?')\n",
    "\n",
    "endpoint_in_service_rule = ChoiceRule.StringEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_status'], value = 'InService')\n",
    "check_endpoint_status_step.add_choice(rule = endpoint_in_service_rule, next_step = endpoint_update_step)\n",
    "\n",
    "wait_step = Wait(state_id = f\"Wait Until Endpoint becomes InService\", seconds = 20)\n",
    "wait_step.next(query_endpoint_lambda_step)\n",
    "\n",
    "check_endpoint_status_step.default_choice(next_step = wait_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_endpoint_existence_step = Choice(\n",
    "    'Endpoint Existed?'\n",
    ")\n",
    "\n",
    "endpoint_existed_rule = ChoiceRule.BooleanEquals(variable = query_endpoint_lambda_step.output()['Payload']['endpoint_existed'], value = True)\n",
    "check_endpoint_existence_step.add_choice(rule = endpoint_existed_rule, next_step = check_endpoint_status_step)\n",
    "\n",
    "check_endpoint_existence_step.default_choice(next_step = endpoint_creation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check endpoint readiness\n",
    "deployed_endpoint_updating_step = Choice('Endpoint is deploying?')\n",
    "\n",
    "wait_deployment_step = Wait(state_id = \"Wait Until Endpoint Deployment Completed\", seconds = 20)\n",
    "wait_deployment_step.next(deployed_endpoint_completed_lambda_step)\n",
    "\n",
    "deployed_endpoint_updating_rule = ChoiceRule.StringEquals(variable = deployed_endpoint_completed_lambda_step.output()['Payload']['endpoint_status'], value = 'Updating')\n",
    "deployed_endpoint_updating_step.add_choice(rule = deployed_endpoint_updating_rule, next_step = wait_deployment_step)\n",
    "\n",
    "final_step = Pass(state_id = 'Pass Step')\n",
    "\n",
    "deployed_endpoint_updating_step.default_choice(next_step = final_step)\n",
    "\n",
    "deployed_endpoint_completed_lambda_step.next(deployed_endpoint_updating_step)\n",
    "endpoint_creation_step.next(deployed_endpoint_completed_lambda_step)\n",
    "endpoint_update_step.next(deployed_endpoint_completed_lambda_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Workflow Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Fail` state to mark the workflow failed in case any of the steps fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_pipeline_step_failure = Fail(\n",
    "    \"ML Workflow Failed\", cause = \"SageMakerPipelineStepFailed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = Chain([training_step, model_step, endpoint_config_step, query_endpoint_lambda_step, check_endpoint_existence_step])\n",
    "deploy_existing_model_path = Chain([existing_model_step, endpoint_config_step, query_endpoint_lambda_step, check_endpoint_existence_step])\n",
    "\n",
    "tuning_step.next(query_hpo_job_lambda_step)\n",
    "query_hpo_job_lambda_step.next(hpo_job_sns_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choice Step Configuration\n",
    "\n",
    "Now, we need to setup choice state for choose HPO / Training or not. See *Choice Rules* in the [AWS Step Functions Data Science SDK documentation](https://aws-step-functions-data-science-sdk.readthedocs.io) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_choice = Choice(\n",
    "    \"To Do HPO?\"\n",
    ")\n",
    "training_choice = Choice(\n",
    "    \"To Do Model Training?\"\n",
    ")\n",
    "\n",
    "# refer to execution input variable with required format - not user friendly.\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = True),\n",
    "    next_step = tuning_step                 \n",
    ")\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = False),\n",
    "    next_step = training_choice\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = True),\n",
    "    next_step = training_path\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = False),\n",
    "    next_step = deploy_existing_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Handling in the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_state_processing = Catch(\n",
    "    error_equals = [\"States.TaskFailed\"],\n",
    "    next_step = failed_state_sagemaker_pipeline_step_failure   \n",
    ")\n",
    "processing_step.add_catch(catch_state_processing)\n",
    "tuning_step.add_catch(catch_state_processing)\n",
    "training_step.add_catch(catch_state_processing)\n",
    "model_step.add_catch(catch_state_processing)\n",
    "endpoint_config_step.add_catch(catch_state_processing)\n",
    "endpoint_creation_step.add_catch(catch_state_processing)\n",
    "endpoint_update_step.add_catch(catch_state_processing)\n",
    "existing_model_step.add_catch(catch_state_processing)\n",
    "query_endpoint_lambda_step.add_catch(catch_state_processing)\n",
    "deployed_endpoint_completed_lambda_step.add_catch(catch_state_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and execute the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = datetime.now().strftime(\"%y%m%d-%H%M\")\n",
    "\n",
    "# execution input parameter values\n",
    "preprocessing_job_name = f\"dm-preprocessing-{uuid.uuid1().hex}\"\n",
    "tuning_job_name = f\"dm-tuning-{suffix}\"\n",
    "training_job_name = f\"dm-training-{uuid.uuid1().hex}\"\n",
    "model_job_name = f\"dm-model-{suffix}\"\n",
    "endpoint_job_name = f\"dm-endpoint-manual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "WORKFLOW_NAME = \"manaul-dm-ml-pipeline\"\n",
    "TO_DO_HPO = False\n",
    "TO_DO_TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfn_client = boto3.client('stepfunctions')\n",
    "\n",
    "workflow_role_arn = f\"arn:aws:states:{region}:{account_id}:stateMachine:{WORKFLOW_NAME}\"\n",
    "\n",
    "try:\n",
    "    response = sfn_client.describe_state_machine(\n",
    "        stateMachineArn = workflow_role_arn\n",
    "    )\n",
    "    existing_workflow = True\n",
    "except: \n",
    "    existing_workflow = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_graph = Chain([processing_step, hpo_choice])\n",
    "# workflow_graph = Chain([hpo_choice])\n",
    "if existing_workflow:\n",
    "    # To update SFN workflow, need to do 'attach' & 'update' together.\n",
    "    workflow = Workflow.attach(state_machine_arn = workflow_role_arn)\n",
    "    workflow.update(definition = workflow_graph, role = WORKFLOW_EXECUTION_ROLE) \n",
    "    # Wait for 10s so that the update is completed before executing workflow\n",
    "    time.sleep(10)\n",
    "else:\n",
    "    workflow = Workflow(\n",
    "        name = WORKFLOW_NAME,\n",
    "        definition = workflow_graph,\n",
    "        role = WORKFLOW_EXECUTION_ROLE\n",
    "    )\n",
    "    workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute workflow\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": TO_DO_HPO,\n",
    "        \"ToDoTraining\": TO_DO_TRAINING,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"LambdaFunctionNameOfQueryEndpoint\": lambda_function_query_endpoint,\n",
    "        \"LambdaFunctionNameOfQueryHpoJob\": lambda_function_query_hpo_job\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state_machine_advice(workflow_name, execution_id):\n",
    "    display(HTML(f'''<br>The Step Function workflow \"{workflow_name}\" is now executing... \n",
    "            <br>To view state machine in the console click \n",
    "            <a target=\"_blank\" href=\"https://{region}.console.aws.amazon.com/states/home?region={region}#/statemachines/view/arn:aws:states:ap-southeast-2:{account_id}:stateMachine:{workflow_name}\">State Machine</a> \n",
    "            <br>To view execution in the console click \n",
    "            <a target=\"_blank\" href=\"https://{region}.console.aws.amazon.com/states/home?region={region}#/executions/details/arn:aws:states:ap-southeast-2:{account_id}:execution:{workflow_name}:{execution_id}\">Execution</a>.\n",
    "        '''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execution.describe()\n",
    "execution_id = response['name']\n",
    "# advice state machine console link\n",
    "display_state_machine_advice(WORKFLOW_NAME, execution_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell multiple times to observe the workflow execution progress. Please note that the execution may take 15-20mins with using existing model for batch transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress(portrait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
